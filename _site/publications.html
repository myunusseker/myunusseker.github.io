<!DOCTYPE html>
<html lang="en">
<!-- Template: https://github.com/luost26/academic-homepage -->
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title>Publications - M. Yunus Seker</title>

    <!-- Stylesheets -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.6.0/css/bootstrap.min.css" integrity="sha512-P5MgMn1jBN01asBgU0z60Qk4QxiXo86+wlFahKrsQf37c9cro517WzVSPPV1tDKzhku2iJ2FVgL67wG03SGnNA==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha512-DTOQO9RWCH3ppGqcWaEA1BIZOC6xxalwEsw9c2QQeAIftl+Vegovlnee1c9QX4TctnWMn13TZye+giMm8e2LwA==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.1/css/academicons.min.css" integrity="sha512-b1ASx0WHgVFL5ZQhTgiPWX+68KjS38Jk87jg7pe+qC7q9YkEtFq0z7xCglv7qGIs/68d3mAp+StfC8WKC5SSAg==" crossorigin="anonymous" />
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,300;0,400;0,700;0,900;1,300;1,400;1,700;1,900&family=Raleway:ital,wght@0,300;0,400;0,500;1,300;1,400;1,500&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" integrity="sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+" crossorigin="anonymous">
    <link rel="stylesheet" href="/assets/css/global.css">
</head>
<body class="bg-light" data-spy="scroll" data-target="#navbar-year" data-offset="100">
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
    <div class="container-lg">
        <a class="navbar-brand" href="/"><strong>M. Yunus Seker</strong></a>
        <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
            <i class="fas fa-map"></i> Menu
        </button>

        <div class="collapse navbar-collapse" id="navbarResponsive">
            <ul class="navbar-nav ml-auto">
                
                <li class="nav-item ">
                    <a class="nav-link" href="/">
                      Home
                      üè†
                        
                    </a>
                </li>
                
                <li class="nav-item active">
                    <a class="nav-link" href="/publications">
                      Publications
                      üìÑ
                        
                    </a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="/about">
                      About
                       Meüë§
                        
                    </a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="/fun">
                      Fun
                      üéÆ
                      
                    </a>
                </li>
                
            </ul>
        </div>
    </div>
</nav>

    <div class="container-lg">
        

<div class="row">
    <div class="col-12 col-lg-10">
        
        
        <h2 class="pt-4" id="year-2025">2025</h2>
        <div class="my-0 p-0 bg-white shadow-sm rounded-sm">
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters  border-gray">
        <div class="col-md-3 col-xl-4 mb-md-0 p-md-2"><img data-src="/assets/images/covers/GTA_cover.png" alt="Grounded Task Axes: Zero-Shot Semantic Skill Generalization via Task-Axis Controllers and Visual Foundation Models" class="lazy w-100 rounded-sm" src="/assets/images/empty_300x200.png"></div>
        <div class="col-md-9 col-xl-8 p-3 pl-md-0">
            <h5 class="mt-0 mb-1 font-weight-normal">Grounded Task Axes: Zero-Shot Semantic Skill Generalization via Task-Axis Controllers and Visual Foundation Models</h5>
            <p class="mt-0 mb-0 small"><a class="text-body" target="_blank" href="https://myunusseker.github.io/"><strong>M. Yunus Seker</strong></a>, <span class="text-body">
            Shobhit Aggarwal, </span><span class="text-body">
            Oliver Kroemer</span></p>
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://arxiv.org/pdf/2505.11680">[ArXiv]</a>
                
                
                
                <a target="_blank" href="https://iamlab-cmu.github.io/GTA/static/videos/video.mp4">[Video]</a>
                
                
                
                <a target="_blank" href="https://iamlab-cmu.github.io/GTA/">[Website]</a>
                
                
            </p>
            <p class="mt-0 mb-0 small"><i></i>  <span class="badge badge-pill badge-publication badge-info">Humanoids 2025 - Under Review</span></p>
            <div class="abstract-toggle-container mt-0 mb-0 small text-muted">
              <div class="abstract-content collapsed">
                In this paper, we propose an example-based zero-shot approach to skill transfer. Rather than treating skills as atomic, we decompose skills into a prioritized list of grounded task-axis (GTA) controllers. Each GTAC defines an adaptable controller, such as a position or force controller, along an axis. Importantly, the GTACs are grounded in object key points and axes, e.g., the relative position of a screw head or the axis of its shaft. Zero-shot transfer is thus achieved by finding semantically-similar grounding features on novel target objects. We achieve this example-based grounding of the skills through the use of foundation models, such as SD-DINO, that can detect semantically similar keypoints of objects. We evaluate our framework on real-robot experiments, including screwing, pouring, and spatula scraping tasks, and demonstrate robust and versatile controller transfer for each.
              </div>
              <a href="javascript:void(0);" class="toggle-abstract-link" onclick="toggleCollapse(this)">Show more ¬ª</a>
            </div>
        </div>
    </div>
</div>

<div class="row no-gutters d-md-none  border-gray rounded-top rounded-bottom lazy" data-src="/assets/images/covers/GTA_cover.png">
    <div class="w-100" style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">Grounded Task Axes: Zero-Shot Semantic Skill Generalization via Task-Axis Controllers and Visual Foundation Models</h5>
                <p class="mt-0 mb-0 small"><a class="text-body" target="_blank" href="https://myunusseker.github.io/"><strong>M. Yunus Seker</strong></a>, <span class="text-body">
            Shobhit Aggarwal, </span><span class="text-body">
            Oliver Kroemer</span></p>
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://arxiv.org/pdf/2505.11680">[ArXiv]</a>
                    
                    
                    
                    <a target="_blank" href="https://iamlab-cmu.github.io/GTA/static/videos/video.mp4">[Video]</a>
                    
                    
                    
                    <a target="_blank" href="https://iamlab-cmu.github.io/GTA/">[Website]</a>
                    
                    
                </p>
                <p class="mt-0 mb-0 small"><i></i>  <span class="badge badge-pill badge-publication badge-info">Humanoids 2025 - Under Review</span></p>
                <div class="abstract-toggle-container mt-0 mb-0 small text-muted">
                  <div class="abstract-content collapsed">
                    In this paper, we propose an example-based zero-shot approach to skill transfer. Rather than treating skills as atomic, we decompose skills into a prioritized list of grounded task-axis (GTA) controllers. Each GTAC defines an adaptable controller, such as a position or force controller, along an axis. Importantly, the GTACs are grounded in object key points and axes, e.g., the relative position of a screw head or the axis of its shaft. Zero-shot transfer is thus achieved by finding semantically-similar grounding features on novel target objects. We achieve this example-based grounding of the skills through the use of foundation models, such as SD-DINO, that can detect semantically similar keypoints of objects. We evaluate our framework on real-robot experiments, including screwing, pouring, and spatula scraping tasks, and demonstrate robust and versatile controller transfer for each.
                  </div>
                  <a href="javascript:void(0);" class="toggle-abstract-link" onclick="toggleCollapse(this)">Show more ¬ª</a>
                </div>
            </div>
        </div>
    </div>

</div>

<style>
.abstract-content {
  line-height: 1.5em;
  max-height: calc(1.2em * 5); /* Show 3 lines */
  overflow: hidden;
  transition: max-height 0.4s ease;
}
.abstract-content.expanded {
  max-height: 1000px; /* Arbitrary large value for full text */
}
.toggle-abstract-link {
  display: inline-block;
  margin-top: 0.25rem;
  color: #007bff;
  font-weight: 500;
  cursor: pointer;
}
</style>

<script>
function toggleCollapse(link) {
  const container = link.closest('.abstract-toggle-container');
  const content = container.querySelector('.abstract-content');
  const expanded = content.classList.toggle('expanded');
  link.textContent = expanded ? 'Show less ¬´' : 'Show more ¬ª';
}
</script>
            
        </div>
        
        
        <h2 class="pt-4" id="year-2024">2024</h2>
        <div class="my-0 p-0 bg-white shadow-sm rounded-sm">
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters border-bottom border-gray">
        <div class="col-md-3 col-xl-4 mb-md-0 p-md-2"><img data-src="/assets/images/covers/MDE_cover.png" alt="Leveraging Simulation-Based Model Preconditions for Fast Action Parameter Optimization with Multiple Models" class="lazy w-100 rounded-sm" src="/assets/images/empty_300x200.png"></div>
        <div class="col-md-9 col-xl-8 p-3 pl-md-0">
            <h5 class="mt-0 mb-1 font-weight-normal">Leveraging Simulation-Based Model Preconditions for Fast Action Parameter Optimization with Multiple Models</h5>
            <p class="mt-0 mb-0 small"><a class="text-body" target="_blank" href="https://myunusseker.github.io/"><strong>M. Yunus Seker</strong></a>, <span class="text-body">
            Oliver Kroemer</span></p>
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/10801532">[Paper]</a>
                
                
                
                <a target="_blank" href="https://arxiv.org/abs/2403.11313">[ArXiv]</a>
                
                
                
                <a target="_blank" href="https://www.youtube.com/watch?v=pDm2cYjQqRk">[Video]</a>
                
                
                
                <a target="_blank" href="https://www.youtube.com/watch?v=B2xraDGOdtA">[Presentation]</a>
                
                
            </p>
            <p class="mt-0 mb-0 small"><i></i>   <span class="badge badge-pill badge-publication badge-success">IROS 2024 - Accepted</span> </p>
            <div class="abstract-toggle-container mt-0 mb-0 small text-muted">
              <div class="abstract-content collapsed">
                Optimizing robotic action parameters is a significant challenge for manipulation tasks that demand high levels
 of precision and generalization. Using a model-based approach,
 the robot must quickly reason about the outcomes of different
 actions using a predictive model to find a set of parameters that
 will have the desired effect. The model may need to capture the
 behaviors of rigid and deformable objects, as well as objects
 of various shapes and sizes. Predictive models often need to
 trade-off speed for prediction accuracy and generalization. This
 paper proposes a framework that leverages the strengths of
 multiple predictive models, including analytical, learned, and
 simulation-based models, to enhance the efficiency and accuracy
 of action parameter optimization. Our approach uses Model
 Deviation Estimators (MDEs) to determine the most suitable
 predictive model for any given state-action parameters, allowing
 the robot to select models to make fast and precise predictions.
 We extend the MDE framework by not only learning sim-to-real
 MDEs, but also sim-to-sim MDEs. Our experiments show that
 these sim-to-sim MDEs provide significantly faster parameter
 optimization as well as a basis for efficiently learning sim-toreal MDEs through finetuning. The ease of collecting sim-to-sim
 training data also allows the robot to learn MDEs based directly
 on visual inputs and local material properties.
              </div>
              <a href="javascript:void(0);" class="toggle-abstract-link" onclick="toggleCollapse(this)">Show more ¬ª</a>
            </div>
        </div>
    </div>
</div>

<div class="row no-gutters d-md-none border-bottom border-gray rounded-top  lazy" data-src="/assets/images/covers/MDE_cover.png">
    <div class="w-100" style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">Leveraging Simulation-Based Model Preconditions for Fast Action Parameter Optimization with Multiple Models</h5>
                <p class="mt-0 mb-0 small"><a class="text-body" target="_blank" href="https://myunusseker.github.io/"><strong>M. Yunus Seker</strong></a>, <span class="text-body">
            Oliver Kroemer</span></p>
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/10801532">[Paper]</a>
                    
                    
                    
                    <a target="_blank" href="https://arxiv.org/abs/2403.11313">[ArXiv]</a>
                    
                    
                    
                    <a target="_blank" href="https://www.youtube.com/watch?v=pDm2cYjQqRk">[Video]</a>
                    
                    
                    
                    <a target="_blank" href="https://www.youtube.com/watch?v=B2xraDGOdtA">[Presentation]</a>
                    
                    
                </p>
                <p class="mt-0 mb-0 small"><i></i>   <span class="badge badge-pill badge-publication badge-success">IROS 2024 - Accepted</span> </p>
                <div class="abstract-toggle-container mt-0 mb-0 small text-muted">
                  <div class="abstract-content collapsed">
                    Optimizing robotic action parameters is a significant challenge for manipulation tasks that demand high levels
 of precision and generalization. Using a model-based approach,
 the robot must quickly reason about the outcomes of different
 actions using a predictive model to find a set of parameters that
 will have the desired effect. The model may need to capture the
 behaviors of rigid and deformable objects, as well as objects
 of various shapes and sizes. Predictive models often need to
 trade-off speed for prediction accuracy and generalization. This
 paper proposes a framework that leverages the strengths of
 multiple predictive models, including analytical, learned, and
 simulation-based models, to enhance the efficiency and accuracy
 of action parameter optimization. Our approach uses Model
 Deviation Estimators (MDEs) to determine the most suitable
 predictive model for any given state-action parameters, allowing
 the robot to select models to make fast and precise predictions.
 We extend the MDE framework by not only learning sim-to-real
 MDEs, but also sim-to-sim MDEs. Our experiments show that
 these sim-to-sim MDEs provide significantly faster parameter
 optimization as well as a basis for efficiently learning sim-toreal MDEs through finetuning. The ease of collecting sim-to-sim
 training data also allows the robot to learn MDEs based directly
 on visual inputs and local material properties.
                  </div>
                  <a href="javascript:void(0);" class="toggle-abstract-link" onclick="toggleCollapse(this)">Show more ¬ª</a>
                </div>
            </div>
        </div>
    </div>

</div>

<style>
.abstract-content {
  line-height: 1.5em;
  max-height: calc(1.2em * 5); /* Show 3 lines */
  overflow: hidden;
  transition: max-height 0.4s ease;
}
.abstract-content.expanded {
  max-height: 1000px; /* Arbitrary large value for full text */
}
.toggle-abstract-link {
  display: inline-block;
  margin-top: 0.25rem;
  color: #007bff;
  font-weight: 500;
  cursor: pointer;
}
</style>

<script>
function toggleCollapse(link) {
  const container = link.closest('.abstract-toggle-container');
  const content = container.querySelector('.abstract-content');
  const expanded = content.classList.toggle('expanded');
  link.textContent = expanded ? 'Show less ¬´' : 'Show more ¬ª';
}
</script>
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters  border-gray">
        <div class="col-md-3 col-xl-4 mb-md-0 p-md-2"><img data-src="/assets/images/covers/UCB_cover.png" alt="Estimating material properties of interacting objects using Sum-GP-UCB" class="lazy w-100 rounded-sm" src="/assets/images/empty_300x200.png"></div>
        <div class="col-md-9 col-xl-8 p-3 pl-md-0">
            <h5 class="mt-0 mb-1 font-weight-normal">Estimating material properties of interacting objects using Sum-GP-UCB</h5>
            <p class="mt-0 mb-0 small"><a class="text-body" target="_blank" href="https://myunusseker.github.io/"><strong>M. Yunus Seker</strong></a>, <span class="text-body">
            Oliver Kroemer</span></p>
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/10610129">[Paper]</a>
                
                
                
                <a target="_blank" href="https://arxiv.org/abs/2310.11749">[ArXiv]</a>
                
                
                
                <a target="_blank" href="https://www.youtube.com/watch?v=KtkPGmK_Bno">[Video]</a>
                
                
                
                <a target="_blank" href="https://youtu.be/N5u6z8cPsec">[Presentation]</a>
                
                
            </p>
            <p class="mt-0 mb-0 small"><i></i>   <span class="badge badge-pill badge-publication badge-success">ICRA 2024 - Accepted</span> </p>
            <div class="abstract-toggle-container mt-0 mb-0 small text-muted">
              <div class="abstract-content collapsed">
                Robots need to estimate the material and dynamic properties of objects from observations in order to simulate them accurately. We present a Bayesian optimization approach to identifying the material property parameters of objects based on a set of observations. Our focus is on estimating these properties based on observations of scenes with different sets of interacting objects. We propose an approach that exploits the structure of the reward function by modeling the reward for each observation separately and using only the parameters of the objects in that scene as inputs. The resulting lower-dimensional models generalize better over the parameter space, which in turn results in a faster optimization. To speed up the optimization process further, and reduce the number of simulation runs needed to find good parameter values, we also propose partial evaluations of the reward function, wherein the selected parameters are only evaluated on a subset of real world evaluations. The approach was successfully evaluated on a set of scenes with a wide range of object interactions, and we showed that our method can effectively perform incremental learning without resetting the rewards of the gathered observations.
              </div>
              <a href="javascript:void(0);" class="toggle-abstract-link" onclick="toggleCollapse(this)">Show more ¬ª</a>
            </div>
        </div>
    </div>
</div>

<div class="row no-gutters d-md-none  border-gray  rounded-bottom lazy" data-src="/assets/images/covers/UCB_cover.png">
    <div class="w-100" style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">Estimating material properties of interacting objects using Sum-GP-UCB</h5>
                <p class="mt-0 mb-0 small"><a class="text-body" target="_blank" href="https://myunusseker.github.io/"><strong>M. Yunus Seker</strong></a>, <span class="text-body">
            Oliver Kroemer</span></p>
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/10610129">[Paper]</a>
                    
                    
                    
                    <a target="_blank" href="https://arxiv.org/abs/2310.11749">[ArXiv]</a>
                    
                    
                    
                    <a target="_blank" href="https://www.youtube.com/watch?v=KtkPGmK_Bno">[Video]</a>
                    
                    
                    
                    <a target="_blank" href="https://youtu.be/N5u6z8cPsec">[Presentation]</a>
                    
                    
                </p>
                <p class="mt-0 mb-0 small"><i></i>   <span class="badge badge-pill badge-publication badge-success">ICRA 2024 - Accepted</span> </p>
                <div class="abstract-toggle-container mt-0 mb-0 small text-muted">
                  <div class="abstract-content collapsed">
                    Robots need to estimate the material and dynamic properties of objects from observations in order to simulate them accurately. We present a Bayesian optimization approach to identifying the material property parameters of objects based on a set of observations. Our focus is on estimating these properties based on observations of scenes with different sets of interacting objects. We propose an approach that exploits the structure of the reward function by modeling the reward for each observation separately and using only the parameters of the objects in that scene as inputs. The resulting lower-dimensional models generalize better over the parameter space, which in turn results in a faster optimization. To speed up the optimization process further, and reduce the number of simulation runs needed to find good parameter values, we also propose partial evaluations of the reward function, wherein the selected parameters are only evaluated on a subset of real world evaluations. The approach was successfully evaluated on a set of scenes with a wide range of object interactions, and we showed that our method can effectively perform incremental learning without resetting the rewards of the gathered observations.
                  </div>
                  <a href="javascript:void(0);" class="toggle-abstract-link" onclick="toggleCollapse(this)">Show more ¬ª</a>
                </div>
            </div>
        </div>
    </div>

</div>

<style>
.abstract-content {
  line-height: 1.5em;
  max-height: calc(1.2em * 5); /* Show 3 lines */
  overflow: hidden;
  transition: max-height 0.4s ease;
}
.abstract-content.expanded {
  max-height: 1000px; /* Arbitrary large value for full text */
}
.toggle-abstract-link {
  display: inline-block;
  margin-top: 0.25rem;
  color: #007bff;
  font-weight: 500;
  cursor: pointer;
}
</style>

<script>
function toggleCollapse(link) {
  const container = link.closest('.abstract-toggle-container');
  const content = container.querySelector('.abstract-content');
  const expanded = content.classList.toggle('expanded');
  link.textContent = expanded ? 'Show less ¬´' : 'Show more ¬ª';
}
</script>
            
        </div>
        
        
        <h2 class="pt-4" id="year-2022">2022</h2>
        <div class="my-0 p-0 bg-white shadow-sm rounded-sm">
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters border-bottom border-gray">
        <div class="col-md-3 col-xl-4 mb-md-0 p-md-2"><img data-src="/assets/images/covers/DSIM_cover.png" alt="DeepSym: Deep Symbol Generation and Rule Learning for Planning from Unsupervised Robot Interaction" class="lazy w-100 rounded-sm" src="/assets/images/empty_300x200.png"></div>
        <div class="col-md-9 col-xl-8 p-3 pl-md-0">
            <h5 class="mt-0 mb-1 font-weight-normal">DeepSym: Deep Symbol Generation and Rule Learning for Planning from Unsupervised Robot Interaction</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            Alper Ahmetoglu, </span><a class="text-body" target="_blank" href="https://myunusseker.github.io/"><strong>M. Yunus Seker</strong></a>, <span class="text-body">
            Justus Piater, </span><span class="text-body">
            Erhan Oztop, </span><span class="text-body">
            Emre Ugur</span></p>
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://dl.acm.org/doi/pdf/10.1613/jair.1.13754">[Paper]</a>
                
                
                
                <a target="_blank" href="https://alpera.xyz/static/deepsym_video.mp4">[Video]</a>
                
                
                
                <a target="_blank" href="https://github.com/alper111/DeepSym">[Code]</a>
                
                
            </p>
            <p class="mt-0 mb-0 small"><i></i>   <span class="badge badge-pill badge-publication badge-success">JAIR 2022 - Accepted</span></p>
            <div class="abstract-toggle-container mt-0 mb-0 small text-muted">
              <div class="abstract-content collapsed">
                Symbolic planning and reasoning are powerful tools for robots tackling complex tasks.
 However, the need to manually design the symbols restrict their applicability, especially for
 robots that are expected to act in open-ended environments. Therefore symbol formation
 and rule extraction should be considered part of robot learning, which, when done properly, will offer scalability, flexibility, and robustness. Towards this goal, we propose a novel
 general method that finds action-grounded, discrete object and effect categories and builds
 probabilistic rules over them for non-trivial action planning. Our robot interacts with objects using an initial action repertoire that is assumed to be acquired earlier and observes
 the effects it can create in the environment. To form action-grounded object, effect, and
 relational categories, we employ a binary bottleneck layer in a predictive, deep encoder-decoder network that takes the image of the scene and the action applied as input, and
 generates the resulting effects in the scene in pixel coordinates. After learning, the binary
 latent vector represents action-driven object categories based on the interaction experience
 of the robot. To distill the knowledge represented by the neural network into rules useful
 for symbolic reasoning, a decision tree is trained to reproduce its decoder function. Probabilistic rules are extracted from the decision paths of the tree and are represented in the
 Probabilistic Planning Domain Definition Language (PPDDL), allowing off-the-shelf planners to operate on the knowledge extracted from the sensorimotor experience of the robot.
 The deployment of the proposed approach for a simulated robotic manipulator enabled the
 discovery of discrete representations of object properties such as ‚Äòrollable‚Äô and ‚Äòinsertable‚Äô.
 In turn, the use of these representations as symbols allowed the generation of effective
 plans for achieving goals, such as building towers of the desired height, demonstrating the
 effectiveness of the approach for multi-step object manipulation. Finally, we demonstrate
 that the system is not only restricted to the robotics domain by assessing its applicability
 to the MNIST 8-puzzle domain in which learned symbols allow for the generation of plans
 that move the empty tile into any given position.
              </div>
              <a href="javascript:void(0);" class="toggle-abstract-link" onclick="toggleCollapse(this)">Show more ¬ª</a>
            </div>
        </div>
    </div>
</div>

<div class="row no-gutters d-md-none border-bottom border-gray rounded-top  lazy" data-src="/assets/images/covers/DSIM_cover.png">
    <div class="w-100" style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">DeepSym: Deep Symbol Generation and Rule Learning for Planning from Unsupervised Robot Interaction</h5>
                <p class="mt-0 mb-0 small"><span class="text-body">
            Alper Ahmetoglu, </span><a class="text-body" target="_blank" href="https://myunusseker.github.io/"><strong>M. Yunus Seker</strong></a>, <span class="text-body">
            Justus Piater, </span><span class="text-body">
            Erhan Oztop, </span><span class="text-body">
            Emre Ugur</span></p>
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://dl.acm.org/doi/pdf/10.1613/jair.1.13754">[Paper]</a>
                    
                    
                    
                    <a target="_blank" href="https://alpera.xyz/static/deepsym_video.mp4">[Video]</a>
                    
                    
                    
                    <a target="_blank" href="https://github.com/alper111/DeepSym">[Code]</a>
                    
                    
                </p>
                <p class="mt-0 mb-0 small"><i></i>   <span class="badge badge-pill badge-publication badge-success">JAIR 2022 - Accepted</span></p>
                <div class="abstract-toggle-container mt-0 mb-0 small text-muted">
                  <div class="abstract-content collapsed">
                    Symbolic planning and reasoning are powerful tools for robots tackling complex tasks.
 However, the need to manually design the symbols restrict their applicability, especially for
 robots that are expected to act in open-ended environments. Therefore symbol formation
 and rule extraction should be considered part of robot learning, which, when done properly, will offer scalability, flexibility, and robustness. Towards this goal, we propose a novel
 general method that finds action-grounded, discrete object and effect categories and builds
 probabilistic rules over them for non-trivial action planning. Our robot interacts with objects using an initial action repertoire that is assumed to be acquired earlier and observes
 the effects it can create in the environment. To form action-grounded object, effect, and
 relational categories, we employ a binary bottleneck layer in a predictive, deep encoder-decoder network that takes the image of the scene and the action applied as input, and
 generates the resulting effects in the scene in pixel coordinates. After learning, the binary
 latent vector represents action-driven object categories based on the interaction experience
 of the robot. To distill the knowledge represented by the neural network into rules useful
 for symbolic reasoning, a decision tree is trained to reproduce its decoder function. Probabilistic rules are extracted from the decision paths of the tree and are represented in the
 Probabilistic Planning Domain Definition Language (PPDDL), allowing off-the-shelf planners to operate on the knowledge extracted from the sensorimotor experience of the robot.
 The deployment of the proposed approach for a simulated robotic manipulator enabled the
 discovery of discrete representations of object properties such as ‚Äòrollable‚Äô and ‚Äòinsertable‚Äô.
 In turn, the use of these representations as symbols allowed the generation of effective
 plans for achieving goals, such as building towers of the desired height, demonstrating the
 effectiveness of the approach for multi-step object manipulation. Finally, we demonstrate
 that the system is not only restricted to the robotics domain by assessing its applicability
 to the MNIST 8-puzzle domain in which learned symbols allow for the generation of plans
 that move the empty tile into any given position.
                  </div>
                  <a href="javascript:void(0);" class="toggle-abstract-link" onclick="toggleCollapse(this)">Show more ¬ª</a>
                </div>
            </div>
        </div>
    </div>

</div>

<style>
.abstract-content {
  line-height: 1.5em;
  max-height: calc(1.2em * 5); /* Show 3 lines */
  overflow: hidden;
  transition: max-height 0.4s ease;
}
.abstract-content.expanded {
  max-height: 1000px; /* Arbitrary large value for full text */
}
.toggle-abstract-link {
  display: inline-block;
  margin-top: 0.25rem;
  color: #007bff;
  font-weight: 500;
  cursor: pointer;
}
</style>

<script>
function toggleCollapse(link) {
  const container = link.closest('.abstract-toggle-container');
  const content = container.querySelector('.abstract-content');
  const expanded = content.classList.toggle('expanded');
  link.textContent = expanded ? 'Show less ¬´' : 'Show more ¬ª';
}
</script>
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters  border-gray">
        <div class="col-md-3 col-xl-4 mb-md-0 p-md-2"><img data-src="/assets/images/covers/DMBN_cover.png" alt="Imitation and mirror systems in robots through Deep Modality Blending Networks" class="lazy w-100 rounded-sm" src="/assets/images/empty_300x200.png"></div>
        <div class="col-md-9 col-xl-8 p-3 pl-md-0">
            <h5 class="mt-0 mb-1 font-weight-normal">Imitation and mirror systems in robots through Deep Modality Blending Networks</h5>
            <p class="mt-0 mb-0 small"><a class="text-body" target="_blank" href="https://myunusseker.github.io/"><strong>M. Yunus Seker</strong></a>, <span class="text-body">
            Alper Ahmetoglu, </span><span class="text-body">
            Yukie Nagai, </span><span class="text-body">
            Minoru Asada, </span><span class="text-body">
            Erhan Oztop, </span><span class="text-body">
            Emre Ugur</span></p>
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://www.sciencedirect.com/science/article/pii/S0893608021004329">[Paper]</a>
                
                
                
                <a target="_blank" href="https://arxiv.org/abs/2106.08422">[ArXiv]</a>
                
                
                
                <a target="_blank" href="https://www.youtube.com/watch?v=omusM1FHllY">[Video]</a>
                
                
                
                <a target="_blank" href="https://github.com/myunusseker/Deep-Modality-Blending-Networks">[Code]</a>
                
                
            </p>
            <p class="mt-0 mb-0 small"><i></i>   <span class="badge badge-pill badge-publication badge-success">Neural Networks 2022 - Accepted</span></p>
            <div class="abstract-toggle-container mt-0 mb-0 small text-muted">
              <div class="abstract-content collapsed">
                Learning to interact with the environment not only empowers the agent with manipulation capability
 but also generates information to facilitate building of action understanding and imitation capabilities.
 This seems to be a strategy adopted by biological systems, in particular primates, as evidenced by the
 existence of mirror neurons that seem to be involved in multi-modal action understanding. How to
 benefit from the interaction experience of the robots to enable understanding actions and goals of
 other agents is still a challenging question. In this study, we propose a novel method, deep modality
 blending networks (DMBN), that creates a common latent space from multi-modal experience of a
 robot by blending multi-modal signals with a stochastic weighting mechanism. We show for the first
 time that deep learning, when combined with a novel modality blending scheme, can facilitate action
 recognition and produce structures to sustain anatomical and effect-based imitation capabilities. Our
 proposed system, which is based on conditional neural processes, can be conditioned on any desired
 sensory/motor value at any time step, and can generate a complete multi-modal trajectory consistent
 with the desired conditioning in one-shot by querying the network for all the sampled time points
 in parallel avoiding the accumulation of prediction errors. Based on simulation experiments with
 an arm-gripper robot and an RGB camera, we showed that DMBN could make accurate predictions
 about any missing modality (camera or joint angles) given the available ones outperforming recent
 multimodal variational autoencoder models in terms of long-horizon high-dimensional trajectory
 predictions. We further showed that given desired images from different perspectives, i.e. images
 generated by the observation of other robots placed on different sides of the table, our system
 could generate image and joint angle sequences that correspond to either anatomical or effect-based
 imitation behavior. To achieve this mirror-like behavior, our system does not perform a pixel-based
 template matching but rather benefits from and relies on the common latent space constructed by
 using both joint and image modalities, as shown by additional experiments. Moreover, we showed that
 mirror learning (in our system) does not only depend on visual experience and cannot be achieved
 without proprioceptive experience. Our experiments showed that out of ten training scenarios with
 different initial configurations, the proposed DMBN model could achieve mirror learning in all of
 the cases where the model that only uses visual information failed in half of them. Overall, the
 proposed DMBN architecture not only serves as a computational model for sustaining mirror neuronlike capabilities, but also stands as a powerful machine learning architecture for high-dimensional
 multi-modal temporal data with robust retrieval capabilities operating with partial information in one
 or multiple modalities.
              </div>
              <a href="javascript:void(0);" class="toggle-abstract-link" onclick="toggleCollapse(this)">Show more ¬ª</a>
            </div>
        </div>
    </div>
</div>

<div class="row no-gutters d-md-none  border-gray  rounded-bottom lazy" data-src="/assets/images/covers/DMBN_cover.png">
    <div class="w-100" style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">Imitation and mirror systems in robots through Deep Modality Blending Networks</h5>
                <p class="mt-0 mb-0 small"><a class="text-body" target="_blank" href="https://myunusseker.github.io/"><strong>M. Yunus Seker</strong></a>, <span class="text-body">
            Alper Ahmetoglu, </span><span class="text-body">
            Yukie Nagai, </span><span class="text-body">
            Minoru Asada, </span><span class="text-body">
            Erhan Oztop, </span><span class="text-body">
            Emre Ugur</span></p>
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://www.sciencedirect.com/science/article/pii/S0893608021004329">[Paper]</a>
                    
                    
                    
                    <a target="_blank" href="https://arxiv.org/abs/2106.08422">[ArXiv]</a>
                    
                    
                    
                    <a target="_blank" href="https://www.youtube.com/watch?v=omusM1FHllY">[Video]</a>
                    
                    
                    
                    <a target="_blank" href="https://github.com/myunusseker/Deep-Modality-Blending-Networks">[Code]</a>
                    
                    
                </p>
                <p class="mt-0 mb-0 small"><i></i>   <span class="badge badge-pill badge-publication badge-success">Neural Networks 2022 - Accepted</span></p>
                <div class="abstract-toggle-container mt-0 mb-0 small text-muted">
                  <div class="abstract-content collapsed">
                    Learning to interact with the environment not only empowers the agent with manipulation capability
 but also generates information to facilitate building of action understanding and imitation capabilities.
 This seems to be a strategy adopted by biological systems, in particular primates, as evidenced by the
 existence of mirror neurons that seem to be involved in multi-modal action understanding. How to
 benefit from the interaction experience of the robots to enable understanding actions and goals of
 other agents is still a challenging question. In this study, we propose a novel method, deep modality
 blending networks (DMBN), that creates a common latent space from multi-modal experience of a
 robot by blending multi-modal signals with a stochastic weighting mechanism. We show for the first
 time that deep learning, when combined with a novel modality blending scheme, can facilitate action
 recognition and produce structures to sustain anatomical and effect-based imitation capabilities. Our
 proposed system, which is based on conditional neural processes, can be conditioned on any desired
 sensory/motor value at any time step, and can generate a complete multi-modal trajectory consistent
 with the desired conditioning in one-shot by querying the network for all the sampled time points
 in parallel avoiding the accumulation of prediction errors. Based on simulation experiments with
 an arm-gripper robot and an RGB camera, we showed that DMBN could make accurate predictions
 about any missing modality (camera or joint angles) given the available ones outperforming recent
 multimodal variational autoencoder models in terms of long-horizon high-dimensional trajectory
 predictions. We further showed that given desired images from different perspectives, i.e. images
 generated by the observation of other robots placed on different sides of the table, our system
 could generate image and joint angle sequences that correspond to either anatomical or effect-based
 imitation behavior. To achieve this mirror-like behavior, our system does not perform a pixel-based
 template matching but rather benefits from and relies on the common latent space constructed by
 using both joint and image modalities, as shown by additional experiments. Moreover, we showed that
 mirror learning (in our system) does not only depend on visual experience and cannot be achieved
 without proprioceptive experience. Our experiments showed that out of ten training scenarios with
 different initial configurations, the proposed DMBN model could achieve mirror learning in all of
 the cases where the model that only uses visual information failed in half of them. Overall, the
 proposed DMBN architecture not only serves as a computational model for sustaining mirror neuronlike capabilities, but also stands as a powerful machine learning architecture for high-dimensional
 multi-modal temporal data with robust retrieval capabilities operating with partial information in one
 or multiple modalities.
                  </div>
                  <a href="javascript:void(0);" class="toggle-abstract-link" onclick="toggleCollapse(this)">Show more ¬ª</a>
                </div>
            </div>
        </div>
    </div>

</div>

<style>
.abstract-content {
  line-height: 1.5em;
  max-height: calc(1.2em * 5); /* Show 3 lines */
  overflow: hidden;
  transition: max-height 0.4s ease;
}
.abstract-content.expanded {
  max-height: 1000px; /* Arbitrary large value for full text */
}
.toggle-abstract-link {
  display: inline-block;
  margin-top: 0.25rem;
  color: #007bff;
  font-weight: 500;
  cursor: pointer;
}
</style>

<script>
function toggleCollapse(link) {
  const container = link.closest('.abstract-toggle-container');
  const content = container.querySelector('.abstract-content');
  const expanded = content.classList.toggle('expanded');
  link.textContent = expanded ? 'Show less ¬´' : 'Show more ¬ª';
}
</script>
            
        </div>
        
        
        <h2 class="pt-4" id="year-2021">2021</h2>
        <div class="my-0 p-0 bg-white shadow-sm rounded-sm">
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters  border-gray">
        <div class="col-md-3 col-xl-4 mb-md-0 p-md-2"><img data-src="/assets/images/covers/ACNMP_cover.png" alt="ACNMP: Skill Transfer and Task Extrapolation through Learning from Demonstration and Reinforcement Learning via Representation Sharing" class="lazy w-100 rounded-sm" src="/assets/images/empty_300x200.png"></div>
        <div class="col-md-9 col-xl-8 p-3 pl-md-0">
            <h5 class="mt-0 mb-1 font-weight-normal">ACNMP: Skill Transfer and Task Extrapolation through Learning from Demonstration and Reinforcement Learning via Representation Sharing</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            M. Tuluhan Akbulut, </span><span class="text-body">
            Erhan Oztop, </span><a class="text-body" target="_blank" href="https://myunusseker.github.io/"><strong>M. Yunus Seker</strong></a>, <span class="text-body">
            Honghu Xue, </span><span class="text-body">
            Ahmet E. Tekden, </span><span class="text-body">
            Emre Ugur</span></p>
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://arxiv.org/abs/2003.11334">[Paper]</a>
                
                
                
                <a target="_blank" href="https://www.youtube.com/watch?v=6ZreL7y3UAY">[Presentation]</a>
                
                
                
                <a target="_blank" href="https://github.com/mtuluhanakbulut/ACNMP">[Code]</a>
                
                
            </p>
            <p class="mt-0 mb-0 small"><i></i>   <span class="badge badge-pill badge-publication badge-success">CoRL 2021 - Accepted</span> <span class="badge badge-pill badge-publication badge-primary">Oral Spotlight</span></p>
            <div class="abstract-toggle-container mt-0 mb-0 small text-muted">
              <div class="abstract-content collapsed">
                To equip robots with dexterous skills, an effective approach is to first transfer the desired skill via Learning from Demonstration (LfD), then let the robot improve it by self-exploration via Reinforcement Learning (RL). In this paper, we propose a novel LfD+ RL framework, namely Adaptive Conditional Neural Movement Primitives (ACNMP), that allows efficient policy improvement in novel environments and effective skill transfer between different agents. This is achieved through exploiting the latent representation learned by the underlying Conditional Neural Process (CNP) model, and simultaneous training of the model with supervised learning (SL) for acquiring the demonstrated trajectories and via RL for new trajectory discovery. Through simulation experiments, we show that (i) ACNMP enables the system to extrapolate to situations where pure LfD fails;(ii) Simultaneous training of the system through SL and RL preserves the shape of demonstrations while adapting to novel situations due to the shared representations used by both learners;(iii) ACNMP enables order-of-magnitude sample-efficient RL in extrapolation of reaching tasks compared to the existing approaches;(iv) ACNMPs can be used to implement skill transfer between robots having different morphology, with competitive learning speeds and importantly with less number of assumptions compared to the state-of-the-art approaches. Finally, we show the real-world suitability of ACNMPs through real robot experiments that involve obstacle avoidance, pick and place and pouring actions.
              </div>
              <a href="javascript:void(0);" class="toggle-abstract-link" onclick="toggleCollapse(this)">Show more ¬ª</a>
            </div>
        </div>
    </div>
</div>

<div class="row no-gutters d-md-none  border-gray rounded-top rounded-bottom lazy" data-src="/assets/images/covers/ACNMP_cover.png">
    <div class="w-100" style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">ACNMP: Skill Transfer and Task Extrapolation through Learning from Demonstration and Reinforcement Learning via Representation Sharing</h5>
                <p class="mt-0 mb-0 small"><span class="text-body">
            M. Tuluhan Akbulut, </span><span class="text-body">
            Erhan Oztop, </span><a class="text-body" target="_blank" href="https://myunusseker.github.io/"><strong>M. Yunus Seker</strong></a>, <span class="text-body">
            Honghu Xue, </span><span class="text-body">
            Ahmet E. Tekden, </span><span class="text-body">
            Emre Ugur</span></p>
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://arxiv.org/abs/2003.11334">[Paper]</a>
                    
                    
                    
                    <a target="_blank" href="https://www.youtube.com/watch?v=6ZreL7y3UAY">[Presentation]</a>
                    
                    
                    
                    <a target="_blank" href="https://github.com/mtuluhanakbulut/ACNMP">[Code]</a>
                    
                    
                </p>
                <p class="mt-0 mb-0 small"><i></i>   <span class="badge badge-pill badge-publication badge-success">CoRL 2021 - Accepted</span> <span class="badge badge-pill badge-publication badge-primary">Oral Spotlight</span></p>
                <div class="abstract-toggle-container mt-0 mb-0 small text-muted">
                  <div class="abstract-content collapsed">
                    To equip robots with dexterous skills, an effective approach is to first transfer the desired skill via Learning from Demonstration (LfD), then let the robot improve it by self-exploration via Reinforcement Learning (RL). In this paper, we propose a novel LfD+ RL framework, namely Adaptive Conditional Neural Movement Primitives (ACNMP), that allows efficient policy improvement in novel environments and effective skill transfer between different agents. This is achieved through exploiting the latent representation learned by the underlying Conditional Neural Process (CNP) model, and simultaneous training of the model with supervised learning (SL) for acquiring the demonstrated trajectories and via RL for new trajectory discovery. Through simulation experiments, we show that (i) ACNMP enables the system to extrapolate to situations where pure LfD fails;(ii) Simultaneous training of the system through SL and RL preserves the shape of demonstrations while adapting to novel situations due to the shared representations used by both learners;(iii) ACNMP enables order-of-magnitude sample-efficient RL in extrapolation of reaching tasks compared to the existing approaches;(iv) ACNMPs can be used to implement skill transfer between robots having different morphology, with competitive learning speeds and importantly with less number of assumptions compared to the state-of-the-art approaches. Finally, we show the real-world suitability of ACNMPs through real robot experiments that involve obstacle avoidance, pick and place and pouring actions.
                  </div>
                  <a href="javascript:void(0);" class="toggle-abstract-link" onclick="toggleCollapse(this)">Show more ¬ª</a>
                </div>
            </div>
        </div>
    </div>

</div>

<style>
.abstract-content {
  line-height: 1.5em;
  max-height: calc(1.2em * 5); /* Show 3 lines */
  overflow: hidden;
  transition: max-height 0.4s ease;
}
.abstract-content.expanded {
  max-height: 1000px; /* Arbitrary large value for full text */
}
.toggle-abstract-link {
  display: inline-block;
  margin-top: 0.25rem;
  color: #007bff;
  font-weight: 500;
  cursor: pointer;
}
</style>

<script>
function toggleCollapse(link) {
  const container = link.closest('.abstract-toggle-container');
  const content = container.querySelector('.abstract-content');
  const expanded = content.classList.toggle('expanded');
  link.textContent = expanded ? 'Show less ¬´' : 'Show more ¬ª';
}
</script>
            
        </div>
        
        
        <h2 class="pt-4" id="year-2020">2020</h2>
        <div class="my-0 p-0 bg-white shadow-sm rounded-sm">
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters  border-gray">
        <div class="col-md-3 col-xl-4 mb-md-0 p-md-2"><img data-src="/assets/images/covers/BRDPN_cover.png" alt="Belief Regulated Dual Propagation Nets for Learning Action Effects on Groups of Articulated Objects" class="lazy w-100 rounded-sm" src="/assets/images/empty_300x200.png"></div>
        <div class="col-md-9 col-xl-8 p-3 pl-md-0">
            <h5 class="mt-0 mb-1 font-weight-normal">Belief Regulated Dual Propagation Nets for Learning Action Effects on Groups of Articulated Objects</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            Ahmet E. Tekden, </span><span class="text-body">
            Aykut Erdem, </span><span class="text-body">
            Erkut Erdem, </span><span class="text-body">
            Mert Imre, </span><a class="text-body" target="_blank" href="https://myunusseker.github.io/"><strong>M. Yunus Seker</strong></a>, <span class="text-body">
            Emre Ugur</span></p>
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://arxiv.org/pdf/1909.03785">[Paper]</a>
                
                
                
                <a target="_blank" href="https://www.youtube.com/watch?v=uWPr7IFT_9k">[Video]</a>
                
                
                
                <a target="_blank" href="https://fzaero.github.io/BRDPN/">[Website]</a>
                
                
            </p>
            <p class="mt-0 mb-0 small"><i></i>   <span class="badge badge-pill badge-publication badge-success">IROS 2020 - Accepted</span> </p>
            <div class="abstract-toggle-container mt-0 mb-0 small text-muted">
              <div class="abstract-content collapsed">
                In recent years, graph neural networks have been
 successfully applied for learning the dynamics of complex and
 partially observable physical systems. However, their use in the
 robotics domain is, to date, still limited. In this paper, we introduce Belief Regulated Dual Propagation Networks (BRDPN), a
 general-purpose learnable physics engine, which enables a robot
 to predict the effects of its actions in scenes containing groups
 of articulated multi-part objects. Specifically, our framework
 extends recently proposed propagation networks (PropNets)
 and consists of two complementary components, a physics
 predictor and a belief regulator. While the former predicts the
 future states of the object(s) manipulated by the robot, the latter
 constantly corrects the robots knowledge regarding the objects
 and their relations. Our results showed that after training in
 a simulator, the robot can reliably predict the consequences
 of its actions in object trajectory level and exploit its own
 interaction experience to correct its belief about the state of the
 environment, enabling better predictions in partially observable
 environments. Furthermore, the trained model was transferred
 to the real world and verified in predicting trajectories of
 pushed interacting objects whose joint relations were initially
 unknown. We compared BRDPN against PropNets, and showed
 that BRDPN performs consistently well. Moreover, BRDPN can
 adapt its physic predictions, since the relations can be predicted
 online.
              </div>
              <a href="javascript:void(0);" class="toggle-abstract-link" onclick="toggleCollapse(this)">Show more ¬ª</a>
            </div>
        </div>
    </div>
</div>

<div class="row no-gutters d-md-none  border-gray rounded-top rounded-bottom lazy" data-src="/assets/images/covers/BRDPN_cover.png">
    <div class="w-100" style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">Belief Regulated Dual Propagation Nets for Learning Action Effects on Groups of Articulated Objects</h5>
                <p class="mt-0 mb-0 small"><span class="text-body">
            Ahmet E. Tekden, </span><span class="text-body">
            Aykut Erdem, </span><span class="text-body">
            Erkut Erdem, </span><span class="text-body">
            Mert Imre, </span><a class="text-body" target="_blank" href="https://myunusseker.github.io/"><strong>M. Yunus Seker</strong></a>, <span class="text-body">
            Emre Ugur</span></p>
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://arxiv.org/pdf/1909.03785">[Paper]</a>
                    
                    
                    
                    <a target="_blank" href="https://www.youtube.com/watch?v=uWPr7IFT_9k">[Video]</a>
                    
                    
                    
                    <a target="_blank" href="https://fzaero.github.io/BRDPN/">[Website]</a>
                    
                    
                </p>
                <p class="mt-0 mb-0 small"><i></i>   <span class="badge badge-pill badge-publication badge-success">IROS 2020 - Accepted</span> </p>
                <div class="abstract-toggle-container mt-0 mb-0 small text-muted">
                  <div class="abstract-content collapsed">
                    In recent years, graph neural networks have been
 successfully applied for learning the dynamics of complex and
 partially observable physical systems. However, their use in the
 robotics domain is, to date, still limited. In this paper, we introduce Belief Regulated Dual Propagation Networks (BRDPN), a
 general-purpose learnable physics engine, which enables a robot
 to predict the effects of its actions in scenes containing groups
 of articulated multi-part objects. Specifically, our framework
 extends recently proposed propagation networks (PropNets)
 and consists of two complementary components, a physics
 predictor and a belief regulator. While the former predicts the
 future states of the object(s) manipulated by the robot, the latter
 constantly corrects the robots knowledge regarding the objects
 and their relations. Our results showed that after training in
 a simulator, the robot can reliably predict the consequences
 of its actions in object trajectory level and exploit its own
 interaction experience to correct its belief about the state of the
 environment, enabling better predictions in partially observable
 environments. Furthermore, the trained model was transferred
 to the real world and verified in predicting trajectories of
 pushed interacting objects whose joint relations were initially
 unknown. We compared BRDPN against PropNets, and showed
 that BRDPN performs consistently well. Moreover, BRDPN can
 adapt its physic predictions, since the relations can be predicted
 online.
                  </div>
                  <a href="javascript:void(0);" class="toggle-abstract-link" onclick="toggleCollapse(this)">Show more ¬ª</a>
                </div>
            </div>
        </div>
    </div>

</div>

<style>
.abstract-content {
  line-height: 1.5em;
  max-height: calc(1.2em * 5); /* Show 3 lines */
  overflow: hidden;
  transition: max-height 0.4s ease;
}
.abstract-content.expanded {
  max-height: 1000px; /* Arbitrary large value for full text */
}
.toggle-abstract-link {
  display: inline-block;
  margin-top: 0.25rem;
  color: #007bff;
  font-weight: 500;
  cursor: pointer;
}
</style>

<script>
function toggleCollapse(link) {
  const container = link.closest('.abstract-toggle-container');
  const content = container.querySelector('.abstract-content');
  const expanded = content.classList.toggle('expanded');
  link.textContent = expanded ? 'Show less ¬´' : 'Show more ¬ª';
}
</script>
            
        </div>
        
        
        <h2 class="pt-4" id="year-2019">2019</h2>
        <div class="my-0 p-0 bg-white shadow-sm rounded-sm">
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters border-bottom border-gray">
        <div class="col-md-3 col-xl-4 mb-md-0 p-md-2"><img data-src="/assets/images/covers/CNMP_cover.png" alt="Conditional Neural Movement Primitives" class="lazy w-100 rounded-sm" src="/assets/images/empty_300x200.png"></div>
        <div class="col-md-9 col-xl-8 p-3 pl-md-0">
            <h5 class="mt-0 mb-1 font-weight-normal">Conditional Neural Movement Primitives</h5>
            <p class="mt-0 mb-0 small"><a class="text-body" target="_blank" href="https://myunusseker.github.io/"><strong>M. Yunus Seker</strong></a>, <span class="text-body">
            Mert Imre, </span><span class="text-body">
            Justus Piater, </span><span class="text-body">
            Emre Ugur</span></p>
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://roboticsproceedings.org/rss15/p71.pdf">[Paper]</a>
                
                
                
                <a target="_blank" href="https://www.youtube.com/watch?v=Afd5FRrc04I">[Video]</a>
                
                
                
                <a target="_blank" href="https://github.com/myunusseker/CNMP">[Code]</a>
                
                
            </p>
            <p class="mt-0 mb-0 small"><i></i>   <span class="badge badge-pill badge-publication badge-success">RSS 2019 - Accepted</span> </p>
            <div class="abstract-toggle-container mt-0 mb-0 small text-muted">
              <div class="abstract-content collapsed">
                Conditional Neural Movement Primitives (CNMPs)
 is a learning from demonstration framework that is designed as
 a robotic movement learning and generation system built on top
 of a recent deep neural architecture, namely Conditional Neural
 Processes (CNPs). Based on CNPs, CNMPs extract the prior
 knowledge directly from the training data by sampling observations from it, and uses it to predict a conditional distribution
 over any other target points. CNMPs specifically learns complex
 temporal multi-modal sensorimotor relations in connection with
 external parameters and goals; produces movement trajectories
 in joint or task space; and executes these trajectories through a
 high-level feedback control loop. Conditioned with an external
 goal that is encoded in the sensorimotor space of the robot,
 predicted sensorimotor trajectory that is expected to be observed
 during the successful execution of the task is generated by the
 CNMP, and the corresponding motor commands are executed.
 In order to detect and react to unexpected events during
 action execution, CNMP is further conditioned with the actual
 sensor readings in each time-step. Through simulations and real
 robot experiments, we showed that CNMPs can learn the nonlinear relations between low-dimensional parameter spaces and
 complex movement trajectories from few demonstrations; and
 they can also model the associations between high-dimensional
 sensorimotor spaces and complex motions using large number of
 demonstrations. The experiments further showed that even the
 task parameters were not explicitly provided to the system, the
 robot could learn their influence by associating the learned sensorimotor representations with the movement trajectories. The
 robot, for example, learned the influence of object weights and
 shapes through exploiting its sensorimotor space that includes
 proprioception and force measurements; and be able to change
 the movement trajectory on the fly when one  of these factors
 were changed through external intervention.
              </div>
              <a href="javascript:void(0);" class="toggle-abstract-link" onclick="toggleCollapse(this)">Show more ¬ª</a>
            </div>
        </div>
    </div>
</div>

<div class="row no-gutters d-md-none border-bottom border-gray rounded-top  lazy" data-src="/assets/images/covers/CNMP_cover.png">
    <div class="w-100" style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">Conditional Neural Movement Primitives</h5>
                <p class="mt-0 mb-0 small"><a class="text-body" target="_blank" href="https://myunusseker.github.io/"><strong>M. Yunus Seker</strong></a>, <span class="text-body">
            Mert Imre, </span><span class="text-body">
            Justus Piater, </span><span class="text-body">
            Emre Ugur</span></p>
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://roboticsproceedings.org/rss15/p71.pdf">[Paper]</a>
                    
                    
                    
                    <a target="_blank" href="https://www.youtube.com/watch?v=Afd5FRrc04I">[Video]</a>
                    
                    
                    
                    <a target="_blank" href="https://github.com/myunusseker/CNMP">[Code]</a>
                    
                    
                </p>
                <p class="mt-0 mb-0 small"><i></i>   <span class="badge badge-pill badge-publication badge-success">RSS 2019 - Accepted</span> </p>
                <div class="abstract-toggle-container mt-0 mb-0 small text-muted">
                  <div class="abstract-content collapsed">
                    Conditional Neural Movement Primitives (CNMPs)
 is a learning from demonstration framework that is designed as
 a robotic movement learning and generation system built on top
 of a recent deep neural architecture, namely Conditional Neural
 Processes (CNPs). Based on CNPs, CNMPs extract the prior
 knowledge directly from the training data by sampling observations from it, and uses it to predict a conditional distribution
 over any other target points. CNMPs specifically learns complex
 temporal multi-modal sensorimotor relations in connection with
 external parameters and goals; produces movement trajectories
 in joint or task space; and executes these trajectories through a
 high-level feedback control loop. Conditioned with an external
 goal that is encoded in the sensorimotor space of the robot,
 predicted sensorimotor trajectory that is expected to be observed
 during the successful execution of the task is generated by the
 CNMP, and the corresponding motor commands are executed.
 In order to detect and react to unexpected events during
 action execution, CNMP is further conditioned with the actual
 sensor readings in each time-step. Through simulations and real
 robot experiments, we showed that CNMPs can learn the nonlinear relations between low-dimensional parameter spaces and
 complex movement trajectories from few demonstrations; and
 they can also model the associations between high-dimensional
 sensorimotor spaces and complex motions using large number of
 demonstrations. The experiments further showed that even the
 task parameters were not explicitly provided to the system, the
 robot could learn their influence by associating the learned sensorimotor representations with the movement trajectories. The
 robot, for example, learned the influence of object weights and
 shapes through exploiting its sensorimotor space that includes
 proprioception and force measurements; and be able to change
 the movement trajectory on the fly when one  of these factors
 were changed through external intervention.
                  </div>
                  <a href="javascript:void(0);" class="toggle-abstract-link" onclick="toggleCollapse(this)">Show more ¬ª</a>
                </div>
            </div>
        </div>
    </div>

</div>

<style>
.abstract-content {
  line-height: 1.5em;
  max-height: calc(1.2em * 5); /* Show 3 lines */
  overflow: hidden;
  transition: max-height 0.4s ease;
}
.abstract-content.expanded {
  max-height: 1000px; /* Arbitrary large value for full text */
}
.toggle-abstract-link {
  display: inline-block;
  margin-top: 0.25rem;
  color: #007bff;
  font-weight: 500;
  cursor: pointer;
}
</style>

<script>
function toggleCollapse(link) {
  const container = link.closest('.abstract-toggle-container');
  const content = container.querySelector('.abstract-content');
  const expanded = content.classList.toggle('expanded');
  link.textContent = expanded ? 'Show less ¬´' : 'Show more ¬ª';
}
</script>
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters  border-gray">
        <div class="col-md-3 col-xl-4 mb-md-0 p-md-2"><svg class="bubble-visual-hash lazy w-100 rounded-sm" data-bubble-visual-hash="/publications/2019/2019-DETP" viewBox="0 0 300 200"></svg></div>
        <div class="col-md-9 col-xl-8 p-3 pl-md-0">
            <h5 class="mt-0 mb-1 font-weight-normal">Deep Effect Trajectory Prediction in Robot Manipulation</h5>
            <p class="mt-0 mb-0 small"><a class="text-body" target="_blank" href="https://myunusseker.github.io/"><strong>M. Yunus Seker</strong></a>, <span class="text-body">
            Ahmet E. Tekden, </span><span class="text-body">
            Emre Ugur</span></p>
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://www.cmpe.boun.edu.tr/~emre/papers/Seker-2019-ROBOT.pdf">[Paper]</a>
                
                
                
                <a target="_blank" href="https://www.youtube.com/watch?v=dFPOH1C3DeY">[Video]</a>
                
                
            </p>
            <p class="mt-0 mb-0 small"><i></i>   <span class="badge badge-pill badge-publication badge-success">RAS 2019 - Accepted</span> </p>
            <div class="abstract-toggle-container mt-0 mb-0 small text-muted">
              <div class="abstract-content collapsed">
                Imagining the consequences of one‚Äôs own actions, before and during their
 execution, allows the agents to choose actions based on their simulated performance, and to monitor the progress by comparing observed to simulated
 behavior. In this study, we propose a deep model that enables a robot to
 learn to predict the consequences of its manipulation actions from its own interaction experience on objects of various shapes. Given the top-down image
 of the object, the robot learns to predict the movement trajectory of the object during execution of a lever-up action performed with a screwdriver in a
 physics-based simulator. The prediction is realized in two stages; the system
 first computes a number of features from the object and then generates the
 complete motion trajectory of the center of mass of the object using Long
 Short Term Memory (LSTM) models. In the first step, we investigated use of
 various feature descriptors such as shape context that encodes a distributed
 representation of positions of the object boundary points, unsupervised features that are extracted from autoencoders, Convolutional Neural Network
 (CNN) based features that are conjointly trained with the LSTMs, and finally task-specific supervised features that are engineered to well-encode the
 underlying dynamics of the lever-up action. The models are trained in simulation with objects of varying edge numbers and tested in the simulated and
 the real world. Our deep and generic CNN-based LSTM model outperformed
 the predictors that use unsupervised representations such as shape descriptors or autoencoder features in the simulated test set. Additionally, it was
 shown to generalize well to novel object shapes that were not experienced
 during model training. Finally, our model was shown to perform well in predicting the consequences of lever-up actions generated by a screwdriver that
 was attached to the gripper of the real UR10 robot. We further showed that
 our system can predict qualitatively different trajectories of objects that roll
 off the table or tumble over as the result of lever-up action.
              </div>
              <a href="javascript:void(0);" class="toggle-abstract-link" onclick="toggleCollapse(this)">Show more ¬ª</a>
            </div>
        </div>
    </div>
</div>

<div class="row no-gutters d-md-none  border-gray  rounded-bottom " data-src="">
    <div class="w-100" style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">Deep Effect Trajectory Prediction in Robot Manipulation</h5>
                <p class="mt-0 mb-0 small"><a class="text-body" target="_blank" href="https://myunusseker.github.io/"><strong>M. Yunus Seker</strong></a>, <span class="text-body">
            Ahmet E. Tekden, </span><span class="text-body">
            Emre Ugur</span></p>
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://www.cmpe.boun.edu.tr/~emre/papers/Seker-2019-ROBOT.pdf">[Paper]</a>
                    
                    
                    
                    <a target="_blank" href="https://www.youtube.com/watch?v=dFPOH1C3DeY">[Video]</a>
                    
                    
                </p>
                <p class="mt-0 mb-0 small"><i></i>   <span class="badge badge-pill badge-publication badge-success">RAS 2019 - Accepted</span> </p>
                <div class="abstract-toggle-container mt-0 mb-0 small text-muted">
                  <div class="abstract-content collapsed">
                    Imagining the consequences of one‚Äôs own actions, before and during their
 execution, allows the agents to choose actions based on their simulated performance, and to monitor the progress by comparing observed to simulated
 behavior. In this study, we propose a deep model that enables a robot to
 learn to predict the consequences of its manipulation actions from its own interaction experience on objects of various shapes. Given the top-down image
 of the object, the robot learns to predict the movement trajectory of the object during execution of a lever-up action performed with a screwdriver in a
 physics-based simulator. The prediction is realized in two stages; the system
 first computes a number of features from the object and then generates the
 complete motion trajectory of the center of mass of the object using Long
 Short Term Memory (LSTM) models. In the first step, we investigated use of
 various feature descriptors such as shape context that encodes a distributed
 representation of positions of the object boundary points, unsupervised features that are extracted from autoencoders, Convolutional Neural Network
 (CNN) based features that are conjointly trained with the LSTMs, and finally task-specific supervised features that are engineered to well-encode the
 underlying dynamics of the lever-up action. The models are trained in simulation with objects of varying edge numbers and tested in the simulated and
 the real world. Our deep and generic CNN-based LSTM model outperformed
 the predictors that use unsupervised representations such as shape descriptors or autoencoder features in the simulated test set. Additionally, it was
 shown to generalize well to novel object shapes that were not experienced
 during model training. Finally, our model was shown to perform well in predicting the consequences of lever-up actions generated by a screwdriver that
 was attached to the gripper of the real UR10 robot. We further showed that
 our system can predict qualitatively different trajectories of objects that roll
 off the table or tumble over as the result of lever-up action.
                  </div>
                  <a href="javascript:void(0);" class="toggle-abstract-link" onclick="toggleCollapse(this)">Show more ¬ª</a>
                </div>
            </div>
        </div>
    </div>

</div>

<style>
.abstract-content {
  line-height: 1.5em;
  max-height: calc(1.2em * 5); /* Show 3 lines */
  overflow: hidden;
  transition: max-height 0.4s ease;
}
.abstract-content.expanded {
  max-height: 1000px; /* Arbitrary large value for full text */
}
.toggle-abstract-link {
  display: inline-block;
  margin-top: 0.25rem;
  color: #007bff;
  font-weight: 500;
  cursor: pointer;
}
</style>

<script>
function toggleCollapse(link) {
  const container = link.closest('.abstract-toggle-container');
  const content = container.querySelector('.abstract-content');
  const expanded = content.classList.toggle('expanded');
  link.textContent = expanded ? 'Show less ¬´' : 'Show more ¬ª';
}
</script>
            
        </div>
        
    </div>

    <div class="col-2 d-none d-lg-block">
        <div id="navbar-year" class="nav nav-pills flex-column sticky-top" style="top: 80px">
            
            <a class="nav-link d-block" href="#year-2025">2025</a>
            
            <a class="nav-link d-block" href="#year-2024">2024</a>
            
            <a class="nav-link d-block" href="#year-2022">2022</a>
            
            <a class="nav-link d-block" href="#year-2021">2021</a>
            
            <a class="nav-link d-block" href="#year-2020">2020</a>
            
            <a class="nav-link d-block" href="#year-2019">2019</a>
            
        </div>
    </div>

</div>

    </div>
    <footer class="footer">
    <div class="container-lg">
        <div class="row my-3">
            <div class="col-6">
                <div class="text-muted">
                    <i>Last updated: Jun 2025</i>
                </div>
            </div>
            <div class="col-6">
                <div class="text-right text-muted">
                    <a href="https://github.com/luost26/academic-homepage" target="_blank"><i class="fas fa-pencil-ruler"></i> academic-homepage</a>
                </div>
            </div>
        </div>
    </div>
</footer>


    <!-- Scripts -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery.lazy/1.7.9/jquery.lazy.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.6.0/js/bootstrap.min.js" integrity="sha512-XKa9Hemdy1Ui3KSGgJdgMyYlUg1gM+QhL6cnlyTe2qzMCYm4nAZ1PsVerQzTTXzonUR+dmswHqgJPuwCq1MaAg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/github-buttons/2.14.2/buttons.min.js" integrity="sha512-OYwZx04hKFeFNYrWxIyo3atgGpb+cxU0ENWBZs72X7T9U+NoHPM1ftUn/Mfw7dRDXrqWA6M1wBg6z6fGE32aeA==" crossorigin="anonymous"></script>
    <script src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
    <script src="https://unpkg.com/imagesloaded@5/imagesloaded.pkgd.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js" integrity="sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9Ggm/Yr2r3Dy4lfFg" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
              delimiters: [
                  {left: '$$', right: '$$', display: true},
                  {left: '$', right: '$', display: false}
              ],
              throwOnError : false
            });
        });
    </script>
    <script src="/assets/js/common.js"></script>
    <script src="/assets/js/bubble_visual_hash.js"></script>
</body>
</html>
