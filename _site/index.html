<!DOCTYPE html>
<html lang="en">
<!-- Template: https://github.com/luost26/academic-homepage -->
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title>Homepage - M. Yunus Seker</title>

    <!-- Stylesheets -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.6.0/css/bootstrap.min.css" integrity="sha512-P5MgMn1jBN01asBgU0z60Qk4QxiXo86+wlFahKrsQf37c9cro517WzVSPPV1tDKzhku2iJ2FVgL67wG03SGnNA==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha512-DTOQO9RWCH3ppGqcWaEA1BIZOC6xxalwEsw9c2QQeAIftl+Vegovlnee1c9QX4TctnWMn13TZye+giMm8e2LwA==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.1/css/academicons.min.css" integrity="sha512-b1ASx0WHgVFL5ZQhTgiPWX+68KjS38Jk87jg7pe+qC7q9YkEtFq0z7xCglv7qGIs/68d3mAp+StfC8WKC5SSAg==" crossorigin="anonymous" />
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,300;0,400;0,700;0,900;1,300;1,400;1,700;1,900&family=Raleway:ital,wght@0,300;0,400;0,500;1,300;1,400;1,500&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" integrity="sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+" crossorigin="anonymous">
    <link rel="stylesheet" href="/assets/css/global.css">
</head>
<body class="bg-light" >
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
    <div class="container-lg">
        <a class="navbar-brand" href="/"><strong>M. Yunus Seker</strong></a>
        <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
            <i class="fas fa-map"></i> Menu
        </button>

        <div class="collapse navbar-collapse" id="navbarResponsive">
            <ul class="navbar-nav ml-auto">
                
                <li class="nav-item active">
                    <a class="nav-link" href="/">
                      Home
                      üè†
                        
                    </a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="/publications">
                      Publications
                      üìÑ
                        
                    </a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="/about">
                      About
                       Meüë§
                        
                    </a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="/fun">
                      Fun
                      üéÆ
                      
                    </a>
                </li>
                
            </ul>
        </div>
    </div>
</nav>

    <div class="container-lg">
        <div class="row">
    <div class="col">
        <div class="card border-0 shadow-sm bg-white">
            <div class="card-body p-10">
                <div class="row">
                    <div class="col">
                        
                        <!-- When the screen is wider than md, float the portrait on the right -->
                        <div class="figure float-right ml-4 mb-4 d-none d-md-block">
                            <img 
                                src="/assets/images/photos/im1.jpeg" 
                                class="figure-img img-fluid img-thumbnail" 
                                style="height: 400px;"
                                data-toggle="tooltip" 
                                data-placement="top" 
                                title="Newell-Simon Hall, The Robotics Institute, Carnegie Mellon University">
                        </div>
                        

                        <div class="d-flex">
                            <div class="h1 font-weight-normal">
                                M. Yunus Seker <small></small>
                            </div>
                            
                            <div class="ml-auto d-md-none">
                                <a target="_blank" href="/assets/images/photos/im1.jpeg">
                                    <img src="/assets/images/photos/im1.jpeg" class="rounded-circle" style="height: 48px;" alt="Portrait">
                                </a>
                            </div>
                            
                        </div>
                        <div class="text-profile-position">
                            
                            <img src="/assets/images/badges/CMU.png" alt="Logo" class="inline-badge"/>
                            PhD Student at Carnegie Mellon University, The Robotics Institute<br/>
                            
                        </div>
                        <div class="text-profile-bio mt-3" >
                            <div>
  <!-- Desktop / Wide View -->
  <div class="d-none d-md-block">
    <p>
      Hi! I‚Äôm a Ph.D. student at <a href="https://www.ri.cmu.edu/" target="_blank">The Robotics Institute, 
      Carnegie Mellon University</a>, where I‚Äôm fortunate to be advised by 
      <a href="https://www.ri.cmu.edu/ri-faculty/oliver-kroemer/" target="_blank">Prof. Oliver Kroemer</a>.
    </p>
    <p>
      My research focuses on building intelligent robotic systems that can <strong>learn complex skills and generalize them to new environments with zero to minimal supervision</strong>. 
      I develop algorithms for <code>robot learning</code>, with an emphasis on <strong>skill acquisition and transfer</strong>, 
      <strong>action-effect prediction</strong>, <strong>affordance understanding</strong>, and <strong>learning from demonstration</strong>. 
      I'm particularly interested in combining <code>robot manipulation</code> with <strong>deep learning</strong>, 
      <strong>perception</strong>, <strong>foundational models</strong>, <strong>LLM/VLMs</strong>, 
      <strong>symbolic reasoning</strong> and <strong>data-efficient optimization techniques</strong> to enable robots to adapt quickly and robustly to real-world scenarios.
    </p>
    <p>
      Ultimately, my goal is to bridge the gap between low-level control and high-level reasoning‚Äî
      empowering robots to understand, plan, and act in ways that are as versatile and intuitive as humans.
    </p>
  </div>

  <!-- Mobile / Narrow View -->
  <div class="d-md-none">
    <p>
      Hi! I am a Ph.D. student at <a href="https://www.ri.cmu.edu/" target="_blank">The Robotics Institute, Carnegie Mellon University</a> working with 
      <a href="https://www.ri.cmu.edu/ri-faculty/oliver-kroemer/" target="_blank">Prof. Oliver Kroemer</a>. I focus on <strong>robot learning</strong>, 
      including <strong>skill transfer</strong>, <strong>affordances</strong>, and 
      <strong>foundational models</strong> for intelligent manipulation. My goal is to help robots learn complex tasks 
      with <strong>minimal supervision</strong> and generalize them to new scenarios.
    </p>
  </div>
</div>
                        </div>
                        
                    </div>
        
                </div>
                <hr />
                <div class="row">
                    <div class="col">

                        <a class="pr-3 no-break" target="_blank" href="https://scholar.google.com/citations?user=H8NkqvQAAAAJ">
                            <i class="fab fa-google-scholar"></i> Google Scholar
                        </a>
                        
                        
                        
                        <a class="pr-3 no-break" target="_blank" href="https://github.com/myunusseker">
                            <i class="fab fa-github"></i> GitHub
                        </a>
                        

                        

                        
                        <a class="pr-3 no-break" target="_blank" href="https://twitter.com/myunusseker">
                            <i class="fab fa-twitter"></i> Twitter
                        </a>
                        

                        

                        
                        <a class="pr-3 no-break" href="mailto:%6D%73%65%6B%65%72@%61%6E%64%72%65%77.%63%6D%75.%65%64%75">
                            <i class="fas fa-at"></i> <span class="email-text">mseker(at)andrew.cmu.edu</span>
                        </a>
                    </div>
                </div>
            </div>
        </div>        
    </div>
</div>




<div id="ghpages-domain-debug-message" class="row mt-3 d-none">
    <div class="col">
        <div class="alert alert-warning my-0" role="alert">
            <h5><i class="fas fa-exclamation-triangle"></i> Warning</h5>
            <strong>Problem:</strong>
            The current name of your GitHub Pages repository ("<code class="repo-name-text"></code>") does not match the recommended repository name for your site ("<code class="domain-name-text"></code>").
            <br>
            <strong>Solution:</strong> Please consider renaming the repository to "<code class="domain-name-text"></code>", so that your site can be accessed directly at "<code>http://<span class="domain-name-text"></span></code>".
            However, if the current repository name is intended, you can ignore this message by removing "<code>{% include widgets/debug_repo_name.html %}</code>" in <code>index.html</code>.
        </div>
    </div>
</div>

<script>
striped_pathname = location.pathname.replace(/\/+$/, "");
document.querySelectorAll(".repo-name-text").forEach(function(element){
    element.textContent = striped_pathname.replace("/", "");
});
document.querySelectorAll(".domain-name-text").forEach(function(element){
    element.textContent = location.hostname;
});

if (location.hostname.search("github.io") != -1 && striped_pathname != ""){
    document.getElementById("ghpages-domain-debug-message").classList.remove('d-none');
}
</script>


<div id="url-debug-message" class="row mt-3 d-none">
    <div class="col">
        <div class="alert alert-warning my-0" role="alert">
            <h5><i class="fas fa-exclamation-triangle"></i> Action required</h5>
            <strong>Problem:</strong>
            The current root path of this site is "<code class="root-path-text"></code>",
            which does not match the <code>baseurl</code> ("<code></code>") configured in <code>_config.yml</code>.
            <br>

            <strong>Solution:</strong>
            Please set the <code>baseurl</code> in <code>_config.yml</code> to "<code class="root-path-text"></code>".
        </div>
    </div>
</div>

<script>
striped_pathname = location.pathname.replace(/\/+$/, "");
if(striped_pathname != ""){
    document.getElementById("url-debug-message").classList.remove('d-none');
}
document.querySelectorAll(".root-path-text").forEach(function(element){
    element.textContent = striped_pathname;
});
</script>



    <div class="row mt-3">
    <div class="col">
        <div class="card border-0 shadow-sm bg-white">
            <div class="card-body p-4">
                <div class="row" data-masonry='{"percentPosition": true }'>
                    
                    <div class="col-md-6">
                        <div class="mx-2 my-1">
                            <h5>Education</h5>
                            <ul class="list-unstyled mb-1">
                                
                                <li class="media mb-1">
                                    <img src="/assets/images/badges/CMU.png" alt="Carnegie Mellon University" style="width: 18px;" class="mr-1 mt-1">
                                    <div class="media-body">
                                        <div>Carnegie Mellon University</div>
                                        <div class="small"></div>
                                        <div class="small d-flex">
                                            <div>The Robotics Institute, School of Computer Science <br/> Ph.D. Student</div>
                                            <div class="mt-auto ml-auto no-break"><em>Sep. 2022 - present</em></div>
                                        </div>
                                    </div>
                                </li>
                                
                                <li class="media mb-1">
                                    <img src="/assets/images/badges/boun.png" alt="Bogazici University" style="width: 18px;" class="mr-1 mt-1">
                                    <div class="media-body">
                                        <div>Bogazici University</div>
                                        <div class="small"></div>
                                        <div class="small d-flex">
                                            <div>MSc and BSc in Computer Science <br/> Istanbul, Turkiye</div>
                                            <div class="mt-auto ml-auto no-break"><em></em></div>
                                        </div>
                                    </div>
                                </li>
                                
                            </ul>
                        </div>
                    </div>
                    

                    
                    <div class="col-md-6">
                        <div class="mx-2 my-1">
                            <h5>Academic Experience</h5>
                            <ul class="list-unstyled mb-1">
                                
                                <li class="media mb-1">
                                    <img src="/assets/images/badges/CMU.png" alt="Research Assistant, CMU" style="width: 18px;" class="mr-1 mt-1">
                                    <div class="media-body">
                                        <div>Research Assistant, CMU</div>
                                        <div class="small"></div>
                                        <div class="small d-flex">
                                            <div>Intelligent Autonomous Manipulation Lab (<a href="https://iamlab-cmu.github.io/">IAM LAB</a>)</div>
                                            <div class="mt-auto ml-auto no-break"><em>2022 - present</em></div>
                                        </div>
                                    </div>
                                </li>
                                
                                <li class="media mb-1">
                                    <img src="/assets/images/badges/CMU.png" alt="Teaching Assistant" style="width: 18px;" class="mr-1 mt-1">
                                    <div class="media-body">
                                        <div>Teaching Assistant</div>
                                        <div class="small"></div>
                                        <div class="small d-flex">
                                            <div>Carnegie Mellon University </br> Bogazici University</div>
                                            <div class="mt-auto ml-auto no-break"><em>2024 - 2025 </br> 2019 - 2020</em></div>
                                        </div>
                                    </div>
                                </li>
                                
                                <li class="media mb-1">
                                    <img src="/assets/images/badges/tokyo.png" alt="Research Assistant Intern, University of Tokyo" style="width: 18px;" class="mr-1 mt-1">
                                    <div class="media-body">
                                        <div>Research Assistant Intern, University of Tokyo</div>
                                        <div class="small"></div>
                                        <div class="small d-flex">
                                            <div>Cognitive Developmental Robotics Lab (<a href="https://developmental-robotics.jp/en/home/">Nagai LAB</a>)</div>
                                            <div class="mt-auto ml-auto no-break"><em>2020</em></div>
                                        </div>
                                    </div>
                                </li>
                                
                                <li class="media mb-1">
                                    <img src="/assets/images/badges/boun.png" alt="Research Assistant, Bogazici University" style="width: 18px;" class="mr-1 mt-1">
                                    <div class="media-body">
                                        <div>Research Assistant, Bogazici University</div>
                                        <div class="small"></div>
                                        <div class="small d-flex">
                                            <div>Cognition, Learning and Robotics Lab (<a href="https://colors.cmpe.boun.edu.tr/">Colors LAB</a>)</div>
                                            <div class="mt-auto ml-auto no-break"><em>2017 - 2022</em></div>
                                        </div>
                                    </div>
                                </li>
                                
                            </ul>
                        </div>
                    </div>
                    

                    
                    <div class="col-md-6">
                        <div class="mx-2 my-1">
                            <h5>Work Experience</h5>
                            <ul class="list-unstyled mb-1">
                                
                                <li class="media mb-1">
                                    <img src="/assets/images/badges/spiky.jpg" alt="Spiky AI" style="width: 18px;" class="mr-1 mt-1">
                                    <div class="media-body">
                                        <div>Spiky AI</div>
                                        <div class="small"></div>
                                        <div class="small d-flex">
                                            <div>Lead ML Research Engineer</div>
                                            <div class="mt-auto ml-auto no-break"><em>2020-2022</em></div>
                                        </div>
                                    </div>
                                </li>
                                
                            </ul>
                        </div>
                    </div>
                    

                    
                    <div class="col-md-12">
                        <div class="mx-2 my-2">
                            <h5>Projects</h5>
                            <ul class="list-unstyled mb-1">
                                
                                <li class="media mb-1">
                                    <img src="/assets/images/badges/arm.png" alt="ARM/MFI - Grounded Task-Axis Skills for Generalizable Robot Manipulation" style="width: 18px;" class="mr-1 mt-1">
                                    <div class="media-body">
                                        <div>ARM/MFI - Grounded Task-Axis Skills for Generalizable Robot Manipulation</div>
                                        <div class="small"></div>
                                        <div class="small d-flex">
                                            <div><div> <!-- Desktop / Wide View --> <div class="d-none d-md-block">
  <p>
    <span style="font-size: 0.9rem;">
      This project introduces <strong>Grounded Task-Axes (GTA)</strong>, a novel framework for enabling zero-shot robotic skill transfer by modularizing robot actions into interpretable and reusable low-level controllers. Each controller is grounded using object-centric <em>keypoints and axes</em>, allowing robots to align and execute skills across novel tools and scenes without any training.
    </span>
  </p>
  <ul style="font-size: 0.9rem;">
    <li>
      <strong>Modular Controller Design:</strong> Skills like screwing, wiping, and inserting are composed from prioritized task-axis controllers (e.g., <em>PosAlign, AxisAlign, ForceSlide</em>) that operate within each other's nullspaces.
    </li>
    <li>
      <strong>Visual Foundation Models:</strong> To generalize across objects, we use foundation models (e.g., DINOv2, SAM) to find semantic and geometric correspondences between keypoints on example and current objects.
    </li>
    <li>
      <strong>Skill Composition for Manufacturing:</strong> In the MFI-ARM project, we use these skills in the context of the NIST assembly box task, defining lifted skills using abstract axes and grounding them for unseen CADs or image inputs, making the system scalable to new tasks and tools.
    </li>
  </ul>

  <p>
    <span style="font-size: 0.9rem;">
      This project bridges traditional control theory with modern visual reasoning, offering <strong>interpretable, adaptable, and sample-efficient (zero-shot) skill transfer</strong> for real-world manufacturing and manipulation tasks.
    </span>
  </p>
</div>
<!-- Mobile / Narrow View --> <div class="d-md-none">
  <span style="font-size: 0.9rem;">
    This project introduces <strong>Grounded Task-Axes (GTA)</strong>, a zero-shot skill transfer framework that enables robots to generalize manipulation skills across novel tools using visual foundation models and modular task-axis controllers.
  </span>
</div> </div></div>
                                            <div class="mt-auto ml-auto no-break"><em>Jan. 2024 - Ongoing</em></div>
                                        </div>
                                    </div>
                                </li>
                                
                                <li class="media mb-1">
                                    <img src="/assets/images/badges/sony.png" alt="Sony AI - Precise Food Manipulation" style="width: 18px;" class="mr-1 mt-1">
                                    <div class="media-body">
                                        <div>Sony AI - Precise Food Manipulation</div>
                                        <div class="small"></div>
                                        <div class="small d-flex">
                                            <div><div>
  <!-- Desktop / Wide View -->
  <div class="d-none d-md-block">
    <span style="font-size: 0.9rem;"> This project tackled the challenge of <strong>precise food manipulation and plating</strong>, requiring robots to interact with deformable and rigid foods with high accuracy. It had two main components: <br/>
    <ul style="font-size: 0.9rem;">
      <li>
        <strong>Material Property Estimation:</strong> We proposed a <em>Bayesian optimization framework (Sum-GP-UCB)</em> to estimate material properties (e.g., mass, stiffness, Poisson ratio) from real-world interaction scenes.
      </li>
      <li>
        <strong>Multi-Model Planning for Plating:</strong> We built a system that switches among <em>heuristic, learned, and simulation-based predictive models</em> to optimize placement actions using <em>Model Deviation Estimators (MDEs)</em>.
      </li>
    </ul>
    </span>
  </div>

  <!-- Mobile / Narrow View -->
  <div class="d-md-none">
    <span style="font-size: 0.9rem;">
      This project with <strong>Sony AI</strong> developed a multi-modal planning system and used <em>Bayesian optimization</em> to help robots perform precise plating and food manipulation tasks.
    </span>
  </div>
</div></div>
                                            <div class="mt-auto ml-auto no-break"><em>Aug. 2022 - Jan 2024</em></div>
                                        </div>
                                    </div>
                                </li>
                                
                            </ul>
                        </div>
                    </div>
                    

                </div>

            </div>
        </div>
    </div>
</div>




    
    
        <div class="my-3 bg-white shadow-sm rounded-sm">
    <h6 class="p-3 mb-0 border-bottom border-gray"><i class="fas fa-rss"></i> News</h6>
    <div class="px-3 pb-1">
        
        <div class="media py-1 ">
            
            <div class="mr-3 text-muted my-1">2025</div>
            <div class="media-body">
                
                <div class="my-1 d-flex">
                    <div>New preprint uploaded: Grounded Task Axes: Zero-Shot Semantic Skill Generalization via Task-Axis Controllers and Visual Foundation Models ‚Äì available on <a href="https://iamlab-cmu.github.io/GTA/">arXiv</a>! <span class="badge badge-pill badge-info">Preprint</span></div>
                    <div class="ml-auto mt-auto text-muted no-break"><i>May 15</i></div>
                </div>
                
            </div>
        </div>
        
        <div class="media py-1 border-top border-gray">
            
            <div class="mr-3 text-muted my-1">2024</div>
            <div class="media-body">
                
                <div class="my-1 d-flex">
                    <div>Our paper Leveraging Simulation-Based Model Preconditions for Fast Action Parameter Optimization with Multiple Models is accepted for IROS 2024! <span class="badge badge-pill badge-success">Accepted</span></div>
                    <div class="ml-auto mt-auto text-muted no-break"><i>May 28</i></div>
                </div>
                
            </div>
        </div>
        
        <div class="media py-1 border-top border-gray">
            
            <div class="mr-3 text-muted my-1">2023</div>
            <div class="media-body">
                
                <div class="my-1 d-flex">
                    <div>Our paper Estimating material properties of interacting objects using Sum-GP-UCB is accepted for ICRA 2024! <span class="badge badge-pill badge-success">Accepted</span></div>
                    <div class="ml-auto mt-auto text-muted no-break"><i>Nov 28</i></div>
                </div>
                
            </div>
        </div>
        
        <div class="media py-1 border-top border-gray">
            
            <div class="mr-3 text-muted my-1">2022</div>
            <div class="media-body">
                
                <div class="my-1 d-flex">
                    <div>Joined the <a href="https://iamlab-cmu.github.io/">IAM LAB</a> at CMU as a Ph.D. student focused on robot learning for manipulation.</div>
                    <div class="ml-auto mt-auto text-muted no-break"><i>Sep 10</i></div>
                </div>
                
            </div>
        </div>
        
    </div>
</div>

    



    
    
<div class="my-3 p-0 bg-white shadow-sm rounded-sm">
    <h6 class="border-bottom border-gray p-3 mb-0">
        <i class="fas fa-star"></i> Selected Publications 
        <a href="/publications">(view all <i class="fas fa-angle-double-right"></i>)</a>
    </h6>
    
        
<div class="d-none d-md-block">
    <div class="row no-gutters border-bottom border-gray">
        <div class="col-md-3 col-xl-3 mb-md-0 p-md-2"><img data-src="/assets/images/covers/GTA_cover.png" alt="Grounded Task Axes: Zero-Shot Semantic Skill Generalization via Task-Axis Controllers and Visual Foundation Models" class="lazy w-100 rounded-sm" src="/assets/images/empty_300x200.png"></div>
        <div class="col-md-9 col-xl-9 p-3 pl-md-0">
            <h5 class="mt-0 mb-1 font-weight-normal">Grounded Task Axes: Zero-Shot Semantic Skill Generalization via Task-Axis Controllers and Visual Foundation Models</h5>
            <p class="mt-0 mb-0 small"><a class="text-body" target="_blank" href="https://myunusseker.github.io/"><strong>M. Yunus Seker</strong></a>, <span class="text-body">
            Shobhit Aggarwal, </span><span class="text-body">
            Oliver Kroemer</span></p>
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://arxiv.org/pdf/2505.11680">[ArXiv]</a>
                
                
                
                <a target="_blank" href="https://iamlab-cmu.github.io/GTA/static/videos/video.mp4">[Video]</a>
                
                
                
                <a target="_blank" href="https://iamlab-cmu.github.io/GTA/">[Website]</a>
                
                
            </p>
            <p class="mt-0 mb-0 small"><i></i>  <span class="badge badge-pill badge-publication badge-info">Humanoids 2025 - Under Review</span></p>
            <p class="mt-0 mb-0 small text-muted">Grounded Task Axes (GTA) introduces a zero-shot skill transfer framework that enables robots to generalize manipulation tasks to unseen objects by grounding modular controllers (like position, force, and orientation) using vision foundation models. It allows robots to perform complex, multi-step tasks‚Äîsuch as scraping, pouring, or inserting‚Äîwithout training or fine-tuning, by matching semantic keypoints between objects.</p>
        </div>
    </div>
</div>

<div class="row no-gutters d-md-none border-bottom border-gray   lazy" data-src="/assets/images/covers/GTA_cover.png">
    <div class="w-100" style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">Grounded Task Axes: Zero-Shot Semantic Skill Generalization via Task-Axis Controllers and Visual Foundation Models</h5>
                <p class="mt-0 mb-0 small"><a class="text-body" target="_blank" href="https://myunusseker.github.io/"><strong>M. Yunus Seker</strong></a>, <span class="text-body">
            Shobhit Aggarwal, </span><span class="text-body">
            Oliver Kroemer</span></p>
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://arxiv.org/pdf/2505.11680">[ArXiv]</a>
                    
                    
                    
                    <a target="_blank" href="https://iamlab-cmu.github.io/GTA/static/videos/video.mp4">[Video]</a>
                    
                    
                    
                    <a target="_blank" href="https://iamlab-cmu.github.io/GTA/">[Website]</a>
                    
                    
                </p>
                <p class="mt-0 mb-0 small"><i></i>  <span class="badge badge-pill badge-publication badge-info">Humanoids 2025 - Under Review</span></p>
                <p class="mt-0 mb-0 small text-muted">Grounded Task Axes (GTA) introduces a zero-shot skill transfer framework that enables robots to generalize manipulation tasks to unseen objects by grounding modular controllers (like position, force, and orientation) using vision foundation models. It allows robots to perform complex, multi-step tasks‚Äîsuch as scraping, pouring, or inserting‚Äîwithout training or fine-tuning, by matching semantic keypoints between objects.</p>
            </div>
        </div>
    </div>

</div>
    
        
<div class="d-none d-md-block">
    <div class="row no-gutters border-bottom border-gray">
        <div class="col-md-3 col-xl-3 mb-md-0 p-md-2"><img data-src="/assets/images/covers/MDE_cover.png" alt="Leveraging Simulation-Based Model Preconditions for Fast Action Parameter Optimization with Multiple Models" class="lazy w-100 rounded-sm" src="/assets/images/empty_300x200.png"></div>
        <div class="col-md-9 col-xl-9 p-3 pl-md-0">
            <h5 class="mt-0 mb-1 font-weight-normal">Leveraging Simulation-Based Model Preconditions for Fast Action Parameter Optimization with Multiple Models</h5>
            <p class="mt-0 mb-0 small"><a class="text-body" target="_blank" href="https://myunusseker.github.io/"><strong>M. Yunus Seker</strong></a>, <span class="text-body">
            Oliver Kroemer</span></p>
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/10801532">[Paper]</a>
                
                
                
                <a target="_blank" href="https://arxiv.org/abs/2403.11313">[ArXiv]</a>
                
                
                
                <a target="_blank" href="https://www.youtube.com/watch?v=pDm2cYjQqRk">[Video]</a>
                
                
                
                <a target="_blank" href="https://www.youtube.com/watch?v=B2xraDGOdtA">[Presentation]</a>
                
                
            </p>
            <p class="mt-0 mb-0 small"><i></i>   <span class="badge badge-pill badge-publication badge-success">IROS 2024 - Accepted</span> </p>
            <p class="mt-0 mb-0 small text-muted">This paper presents a framework that optimizes robotic actions by choosing between multiple predictive models‚Äîanalytical, learned, and simulation-based‚Äîbased on context. Using Model Deviation Estimators (MDEs), the robot selects the most reliable model to quickly and accurately predict outcomes. The introduction of sim-to-sim MDEs enables faster optimization and smooth transfer to real-world tasks through fine-tuning.</p>
        </div>
    </div>
</div>

<div class="row no-gutters d-md-none border-bottom border-gray   lazy" data-src="/assets/images/covers/MDE_cover.png">
    <div class="w-100" style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">Leveraging Simulation-Based Model Preconditions for Fast Action Parameter Optimization with Multiple Models</h5>
                <p class="mt-0 mb-0 small"><a class="text-body" target="_blank" href="https://myunusseker.github.io/"><strong>M. Yunus Seker</strong></a>, <span class="text-body">
            Oliver Kroemer</span></p>
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/10801532">[Paper]</a>
                    
                    
                    
                    <a target="_blank" href="https://arxiv.org/abs/2403.11313">[ArXiv]</a>
                    
                    
                    
                    <a target="_blank" href="https://www.youtube.com/watch?v=pDm2cYjQqRk">[Video]</a>
                    
                    
                    
                    <a target="_blank" href="https://www.youtube.com/watch?v=B2xraDGOdtA">[Presentation]</a>
                    
                    
                </p>
                <p class="mt-0 mb-0 small"><i></i>   <span class="badge badge-pill badge-publication badge-success">IROS 2024 - Accepted</span> </p>
                <p class="mt-0 mb-0 small text-muted">This paper presents a framework that optimizes robotic actions by choosing between multiple predictive models‚Äîanalytical, learned, and simulation-based‚Äîbased on context. Using Model Deviation Estimators (MDEs), the robot selects the most reliable model to quickly and accurately predict outcomes. The introduction of sim-to-sim MDEs enables faster optimization and smooth transfer to real-world tasks through fine-tuning.</p>
            </div>
        </div>
    </div>

</div>
    
        
<div class="d-none d-md-block">
    <div class="row no-gutters border-bottom border-gray">
        <div class="col-md-3 col-xl-3 mb-md-0 p-md-2"><img data-src="/assets/images/covers/UCB_cover.png" alt="Estimating material properties of interacting objects using Sum-GP-UCB" class="lazy w-100 rounded-sm" src="/assets/images/empty_300x200.png"></div>
        <div class="col-md-9 col-xl-9 p-3 pl-md-0">
            <h5 class="mt-0 mb-1 font-weight-normal">Estimating material properties of interacting objects using Sum-GP-UCB</h5>
            <p class="mt-0 mb-0 small"><a class="text-body" target="_blank" href="https://myunusseker.github.io/"><strong>M. Yunus Seker</strong></a>, <span class="text-body">
            Oliver Kroemer</span></p>
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/10610129">[Paper]</a>
                
                
                
                <a target="_blank" href="https://arxiv.org/abs/2310.11749">[ArXiv]</a>
                
                
                
                <a target="_blank" href="https://www.youtube.com/watch?v=KtkPGmK_Bno">[Video]</a>
                
                
                
                <a target="_blank" href="https://youtu.be/N5u6z8cPsec">[Presentation]</a>
                
                
            </p>
            <p class="mt-0 mb-0 small"><i></i>   <span class="badge badge-pill badge-publication badge-success">ICRA 2024 - Accepted</span> </p>
            <p class="mt-0 mb-0 small text-muted">This paper introduces a Bayesian optimization framework to estimate object material properties from observed interactions. By modeling each observation independently and focusing only on relevant object parameters, the method achieves faster, more generalizable optimization. It further improves efficiency through partial reward evaluations, enabling robust and incremental learning across diverse real-world scenes.</p>
        </div>
    </div>
</div>

<div class="row no-gutters d-md-none border-bottom border-gray   lazy" data-src="/assets/images/covers/UCB_cover.png">
    <div class="w-100" style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">Estimating material properties of interacting objects using Sum-GP-UCB</h5>
                <p class="mt-0 mb-0 small"><a class="text-body" target="_blank" href="https://myunusseker.github.io/"><strong>M. Yunus Seker</strong></a>, <span class="text-body">
            Oliver Kroemer</span></p>
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/10610129">[Paper]</a>
                    
                    
                    
                    <a target="_blank" href="https://arxiv.org/abs/2310.11749">[ArXiv]</a>
                    
                    
                    
                    <a target="_blank" href="https://www.youtube.com/watch?v=KtkPGmK_Bno">[Video]</a>
                    
                    
                    
                    <a target="_blank" href="https://youtu.be/N5u6z8cPsec">[Presentation]</a>
                    
                    
                </p>
                <p class="mt-0 mb-0 small"><i></i>   <span class="badge badge-pill badge-publication badge-success">ICRA 2024 - Accepted</span> </p>
                <p class="mt-0 mb-0 small text-muted">This paper introduces a Bayesian optimization framework to estimate object material properties from observed interactions. By modeling each observation independently and focusing only on relevant object parameters, the method achieves faster, more generalizable optimization. It further improves efficiency through partial reward evaluations, enabling robust and incremental learning across diverse real-world scenes.</p>
            </div>
        </div>
    </div>

</div>
    
        
<div class="d-none d-md-block">
    <div class="row no-gutters border-bottom border-gray">
        <div class="col-md-3 col-xl-3 mb-md-0 p-md-2"><img data-src="/assets/images/covers/CNMP_cover.png" alt="Conditional Neural Movement Primitives" class="lazy w-100 rounded-sm" src="/assets/images/empty_300x200.png"></div>
        <div class="col-md-9 col-xl-9 p-3 pl-md-0">
            <h5 class="mt-0 mb-1 font-weight-normal">Conditional Neural Movement Primitives</h5>
            <p class="mt-0 mb-0 small"><a class="text-body" target="_blank" href="https://myunusseker.github.io/"><strong>M. Yunus Seker</strong></a>, <span class="text-body">
            Mert Imre, </span><span class="text-body">
            Justus Piater, </span><span class="text-body">
            Emre Ugur</span></p>
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://roboticsproceedings.org/rss15/p71.pdf">[Paper]</a>
                
                
                
                <a target="_blank" href="https://www.youtube.com/watch?v=Afd5FRrc04I">[Video]</a>
                
                
                
                <a target="_blank" href="https://github.com/myunusseker/CNMP">[Code]</a>
                
                
            </p>
            <p class="mt-0 mb-0 small"><i></i>   <span class="badge badge-pill badge-publication badge-success">RSS 2019 - Accepted</span> </p>
            <p class="mt-0 mb-0 small text-muted">Conditional Neural Movement Primitives (CNMPs) are a learning-from-demonstration framework that enables robots to generate and adapt complex movement trajectories based on external goals and sensor feedback. Built on Conditional Neural Processes (CNPs), CNMPs learn temporal sensorimotor patterns from demonstrations and produce joint or task-space motions conditioned on goals and real-time sensory input. Experiments show CNMPs can generalize from few or many demonstrations, adapt to factors like object weight or shape, and react to unexpected changes during execution.</p>
        </div>
    </div>
</div>

<div class="row no-gutters d-md-none border-bottom border-gray   lazy" data-src="/assets/images/covers/CNMP_cover.png">
    <div class="w-100" style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">Conditional Neural Movement Primitives</h5>
                <p class="mt-0 mb-0 small"><a class="text-body" target="_blank" href="https://myunusseker.github.io/"><strong>M. Yunus Seker</strong></a>, <span class="text-body">
            Mert Imre, </span><span class="text-body">
            Justus Piater, </span><span class="text-body">
            Emre Ugur</span></p>
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://roboticsproceedings.org/rss15/p71.pdf">[Paper]</a>
                    
                    
                    
                    <a target="_blank" href="https://www.youtube.com/watch?v=Afd5FRrc04I">[Video]</a>
                    
                    
                    
                    <a target="_blank" href="https://github.com/myunusseker/CNMP">[Code]</a>
                    
                    
                </p>
                <p class="mt-0 mb-0 small"><i></i>   <span class="badge badge-pill badge-publication badge-success">RSS 2019 - Accepted</span> </p>
                <p class="mt-0 mb-0 small text-muted">Conditional Neural Movement Primitives (CNMPs) are a learning-from-demonstration framework that enables robots to generate and adapt complex movement trajectories based on external goals and sensor feedback. Built on Conditional Neural Processes (CNPs), CNMPs learn temporal sensorimotor patterns from demonstrations and produce joint or task-space motions conditioned on goals and real-time sensory input. Experiments show CNMPs can generalize from few or many demonstrations, adapt to factors like object weight or shape, and react to unexpected changes during execution.</p>
            </div>
        </div>
    </div>

</div>
    
    <h6 class="d-block p-3 mt-0 text-right">
        <a href="/publications">All publications <i class="fas fa-angle-double-right"></i></a>
    </h6>
</div>




    </div>
    <footer class="footer">
    <div class="container-lg">
        <div class="row my-3">
            <div class="col-6">
                <div class="text-muted">
                    <i>Last updated: Jun 2025</i>
                </div>
            </div>
            <div class="col-6">
                <div class="text-right text-muted">
                    <a href="https://github.com/luost26/academic-homepage" target="_blank"><i class="fas fa-pencil-ruler"></i> academic-homepage</a>
                </div>
            </div>
        </div>
    </div>
</footer>


    <!-- Scripts -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery.lazy/1.7.9/jquery.lazy.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.6.0/js/bootstrap.min.js" integrity="sha512-XKa9Hemdy1Ui3KSGgJdgMyYlUg1gM+QhL6cnlyTe2qzMCYm4nAZ1PsVerQzTTXzonUR+dmswHqgJPuwCq1MaAg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/github-buttons/2.14.2/buttons.min.js" integrity="sha512-OYwZx04hKFeFNYrWxIyo3atgGpb+cxU0ENWBZs72X7T9U+NoHPM1ftUn/Mfw7dRDXrqWA6M1wBg6z6fGE32aeA==" crossorigin="anonymous"></script>
    <script src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
    <script src="https://unpkg.com/imagesloaded@5/imagesloaded.pkgd.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js" integrity="sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9Ggm/Yr2r3Dy4lfFg" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
              delimiters: [
                  {left: '$$', right: '$$', display: true},
                  {left: '$', right: '$', display: false}
              ],
              throwOnError : false
            });
        });
    </script>
    <script src="/assets/js/common.js"></script>
    <script src="/assets/js/bubble_visual_hash.js"></script>
</body>
</html>
