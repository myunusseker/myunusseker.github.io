---
title:          "Imitation and mirror systems in robots through Deep Modality Blending Networks"
date:           2022-02-01 00:01:00 +0800
selected:       false
#pub:            "IROS"
# pub_pre:        "Submitted to "
# pub_post:       'Under review,'
pub_last:       ' <span class="badge badge-pill badge-publication badge-success">Neural Networks 2022 - Accepted</span>'
#pub_date:       "2024"

abstract: >-
   Learning to interact with the environment not only empowers the agent with manipulation capability
    but also generates information to facilitate building of action understanding and imitation capabilities.
    This seems to be a strategy adopted by biological systems, in particular primates, as evidenced by the
    existence of mirror neurons that seem to be involved in multi-modal action understanding. How to
    benefit from the interaction experience of the robots to enable understanding actions and goals of
    other agents is still a challenging question. In this study, we propose a novel method, deep modality
    blending networks (DMBN), that creates a common latent space from multi-modal experience of a
    robot by blending multi-modal signals with a stochastic weighting mechanism. We show for the first
    time that deep learning, when combined with a novel modality blending scheme, can facilitate action
    recognition and produce structures to sustain anatomical and effect-based imitation capabilities. Our
    proposed system, which is based on conditional neural processes, can be conditioned on any desired
    sensory/motor value at any time step, and can generate a complete multi-modal trajectory consistent
    with the desired conditioning in one-shot by querying the network for all the sampled time points
    in parallel avoiding the accumulation of prediction errors. Based on simulation experiments with
    an arm-gripper robot and an RGB camera, we showed that DMBN could make accurate predictions
    about any missing modality (camera or joint angles) given the available ones outperforming recent
    multimodal variational autoencoder models in terms of long-horizon high-dimensional trajectory
    predictions. We further showed that given desired images from different perspectives, i.e. images
    generated by the observation of other robots placed on different sides of the table, our system
    could generate image and joint angle sequences that correspond to either anatomical or effect-based
    imitation behavior. To achieve this mirror-like behavior, our system does not perform a pixel-based
    template matching but rather benefits from and relies on the common latent space constructed by
    using both joint and image modalities, as shown by additional experiments. Moreover, we showed that
    mirror learning (in our system) does not only depend on visual experience and cannot be achieved
    without proprioceptive experience. Our experiments showed that out of ten training scenarios with
    different initial configurations, the proposed DMBN model could achieve mirror learning in all of
    the cases where the model that only uses visual information failed in half of them. Overall, the
    proposed DMBN architecture not only serves as a computational model for sustaining mirror neuronlike capabilities, but also stands as a powerful machine learning architecture for high-dimensional
    multi-modal temporal data with robust retrieval capabilities operating with partial information in one
    or multiple modalities.

short_abstract: This paper presents Deep Modality Blending Networks (DMBN), a novel framework that enables robots to blend multi-modal sensory experiences—such as vision and proprioception—into a shared latent space for action understanding and imitation. Inspired by mirror neuron systems, DMBN uses stochastic blending and Conditional Neural Processes to predict missing modalities and generate full sensory-motor trajectories in one shot. Experiments demonstrate robust anatomical and effect-based imitation from visual cues, outperforming existing multimodal models in long-horizon trajectory prediction.
cover:          /assets/images/covers/DMBN_cover.png
authors:
  - M. Yunus Seker
  - Alper Ahmetoglu
  - Yukie Nagai
  - Minoru Asada
  - Erhan Oztop
  - Emre Ugur

links:
  Paper: https://www.sciencedirect.com/science/article/pii/S0893608021004329
  ArXiv: https://arxiv.org/abs/2106.08422
  Video: https://www.youtube.com/watch?v=omusM1FHllY
  Code: https://github.com/myunusseker/Deep-Modality-Blending-Networks
---
