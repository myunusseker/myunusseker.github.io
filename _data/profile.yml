primary_name: "M. Yunus Seker"
secondary_name: ""
navbar_name: "M. Yunus Seker"

positions:
- logo: /assets/images/badges/CMU.png  # Logo is optional
  name: PhD Student at Carnegie Mellon University, The Robotics Institute

# cv_link: /
gscholar: H8NkqvQAAAAJ  # This is an example, replace it with your own Google Scholar ID
github: myunusseker
twitter: myunusseker # Do not include the '@' symbol
email: "mseker@andrew.cmu.edu"
# wechat_qrcode: /assets/images/etc/wechat.jpg
# wechat_prompt: >-
#   Please tell me your <strong>name</strong> and <strong>affiliation</strong> (current or past) when adding my wechat. Thanks!
# linkedin: your-linked-in-id
# orcid: 0000-0000-0000-0000

short_bio_text_justify: false
short_bio: >-
  <p>
    Hi! I’m a Ph.D. student at <a href="https://www.ri.cmu.edu/" target="_blank">The Robotics Institute, 
    Carnegie Mellon University</a>, where I’m fortunate to be advised by <a href="https://www.ri.cmu.edu/ri-faculty/oliver-kroemer/" target="_blank">Prof. Oliver Kroemer</a>.
  </p>
  <p>
    My research focuses on building intelligent robotic systems that can <strong>learn complex skills and generalize them to new environments with zero to minimal supervision</strong>. I develop algorithms for <strong>robot learning</strong>, with an emphasis on <em>skill acquisition and transfer</em>, <em>action-effect prediction</em>, <em>affordance understanding</em>, and <em>learning from demonstration</em>. I'm particularly interested in combining <strong>robot manipulation</strong> with <em>deep learning</em>, <em>perception</em>, <em>foundational models</em>, <em>LLM/VLMs</em>, <em>symbolic reasoning</em> and <em>data-efficient optimization techniques</em> to enable robots to adapt quickly and robustly to real-world scenarios.
  </p>
  <p>
    Ultimately, my goal is to bridge the gap between low-level control and high-level reasoning—empowering robots to understand, plan, and act in ways that are as versatile and intuitive as humans.
  </p>

portrait_url: /assets/images/photos/im1.jpeg
portrait_caption: >-
  Photo by Manja Vitolic on Unsplash (this caption is optional, comment it out to disable).

education:
- name: Carnegie Mellon University
  logo: /assets/images/badges/CMU.png
  position: >- 
    The Robotics Institute, School of Computer Science <br/>
    Ph.D. Student
  date: Sep. 2022 - present
- name: Bogazici University
  logo: /assets/images/badges/boun.png
  position: >-
    MSc and BSc in Computer Science <br/>
    Istanbul, Turkiye

experience:
 - name: Teaching Assistant, CMU
   logo: /assets/images/badges/CMU.png
   position: Carnegie Mellon University
   date: 2024 - 2025
 - name: Research Assistant, CMU
   logo: /assets/images/badges/CMU.png
   position: Intelligent Autonomous Manipulation Lab (<a href="https://iamlab-cmu.github.io/">IAM LAB</a>)
   date: 2022 - present
 - name: Research Assistant Intern, University of Tokyo
   logo: /assets/images/badges/tokyo.png
   position: Cognitive Developmental Robotics Lab (<a href="https://developmental-robotics.jp/en/home/">Nagai LAB</a>)
   date: 2020
 - name: Teaching Assistant, Bogazici University
   logo: /assets/images/badges/boun.png
   position: Bogazici University
   date: 2019 - 2020
 - name: Research Assistant, Bogazici University
   logo: /assets/images/badges/boun.png
   position: Cognition, Learning and Robotics Lab (<a href="https://colors.cmpe.boun.edu.tr/">Colors LAB</a>)
   date: 2017 - 2022


work:
- name: Spiky AI
  position: Lead ML Research Engineer
  logo: /assets/images/badges/spiky.jpg
  date: 2020-2022

projects:
 - name: ARM/MFI - Grounded Task-Axis Skills for Generalizable Robot Manipulation
   logo: /assets/images/badges/arm.png
   position: >-   
    <p>
      <span style="font-size: 0.9rem;">
        This project introduces <strong>Grounded Task-Axes (GTA)</strong>, a novel framework for enabling zero-shot robotic skill transfer by modularizing robot actions into interpretable and reusable low-level controllers. Each controller is grounded using object-centric <em>keypoints and axes</em>, allowing robots to align and execute skills across novel tools and scenes without any training.
      </span>
    </p>
    
    <ul style="font-size: 0.9rem;">
      <li>
        <strong>Modular Controller Design:</strong> Skills like screwing, wiping, and inserting are composed from prioritized task-axis controllers (e.g., <em>PosAlign, AxisAlign, ForceSlide</em>) that operate within each other's nullspaces.
      </li>
      <li>
        <strong>Visual Foundation Models:</strong> To generalize across objects, we use foundation models (e.g., DINOv2, SAM) to find semantic and geometric correspondences between keypoints on example and current objects.
      </li>
      <li>
        <strong>Skill Composition for Manufacturing:</strong> In the MFI-ARM project, we use these skills in the context of the NIST assembly box task, defining lifted skills using abstract axes and grounding them for unseen CADs or image inputs, making the system scalable to new tasks and tools.
      </li>
    </ul>
    
    <p>
      <span style="font-size: 0.9rem;">
        This project bridges traditional control theory with modern visual reasoning, offering <strong>interpretable, adaptable, and sample-efficient (zero-shot) skill transfer</strong> for real-world manufacturing and manipulation tasks.
      </span>
    </p>
   date: Jan. 2024 - Ongoing
 - name: Sony AI - Precise Food Manipulation
   logo: /assets/images/badges/sony.png
   position: >-
    <span style="font-size: 0.9rem;"> This project tackled the challenge of <strong>precise food manipulation and plating</strong>, requiring robots to interact with deformable and rigid foods with high accuracy. It had two main components: <br/>
    <ul style="font-size: 0.9rem;">
      <li>
        <strong>Material Property Estimation:</strong> We proposed a <em>Bayesian optimization framework (Sum-GP-UCB)</em> to estimate material properties (e.g., mass, stiffness, Poisson ratio) from real-world interaction scenes. By modeling each observation independently and supporting partial reward evaluations, our method achieved faster, more efficient convergence.
      </li>
      <li>
        <strong>Multi-Model Planning for Plating:</strong> We built a system that selects among <em>heuristic, learned, and simulation-based predictive models</em> to optimize placement actions. Using <em>Model Deviation Estimators (MDEs)</em>, the robot could dynamically switch models based on prediction reliability. We introduced <em>sim-to-sim MDEs</em> to pretrain with synthetic data and efficiently adapted them to the real world through finetuning.
      </li>
    </ul>
    </span>
   date: Aug. 2022 - Jan 2024
